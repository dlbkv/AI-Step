{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7394372,"sourceType":"datasetVersion","datasetId":4257126}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[Переломи](https://www.kaggle.com/datasets/pkdarabi/bone-break-classification-image-dataset)\n\n[фрукти](https://www.kaggle.com/datasets/sshikamaru/fruit-recognition)\n\n[лейкемія](https://www.kaggle.com/datasets/andrewmvd/leukemia-classification)","metadata":{}},{"cell_type":"markdown","source":"# Що може комп'ютерний зір?\nКомп'ютерний зір (КЗ) - це захоплююча галузь штучного інтелекту (ШІ), яка дозволяє комп'ютерам інтерпретувати та розуміти візуальну інформацію зі світу. Подібно до того, як люди використовують очі та мозок для сприйняття навколишнього світу, алгоритми КЗ обробляють цифрові зображення та відео для вилучення значущої інформації. \n\nОсь кілька цікавих застосувань комп'ютерного зору:\n\n* **Класифікація зображень:** Розпізнавання об'єктів, сцен і дій на зображеннях (наприклад, класифікація зображення як такого, що містить кота).\n* **Виявлення об'єктів:** Ідентифікація та визначення місцезнаходження конкретних об'єктів на зображеннях або відео (наприклад, виявлення пішоходів на дорозі).\n* **Сегментація зображення:** Поділ зображення на області, що відповідають різним об'єктам або частинам сцени (наприклад, сегментація зображення, щоб відокремити небо від переднього плану).\n* **Створення зображень:** Створення нових зображень або зміна існуючих (наприклад, створення реалістичних зображень облич або редагування фотографій).","metadata":{}},{"cell_type":"markdown","source":"# Що таке тензор зображення?\n\nЗображення - це набір пікселів, де кожен піксель представляє крихітну точку зображення з певним значенням кольору.  Однак комп'ютери обробляють дані більш ефективно, використовуючи математичні структури, такі як тензори.\n\nОсь як зображення представляються у вигляді тензорів у комп'ютерному зорі:\n\n* **Пікселі як елементи:** Кожен піксель зображення стає елементом (значенням) у тензорі.\n* **Розміри:** Тензор зазвичай має три виміри:\n *  **Висота:** Кількість рядків у зображенні.\n * **Ширина:** Кількість стовпців у зображенні.\n * **Канали**: Кількість колірних каналів у зображенні. \n\nПоширені приклади:\n* **Відтінки сірого**: Один канал (значення, що представляють рівні інтенсивності).\n* **RGB (червоний, зелений, синій):** Три канали, по одному для кожного основного кольору, об'єднані для створення різних кольорів.\n\n![](https://res.cloudinary.com/practicaldev/image/fetch/s--jSCQ5g1J--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.researchgate.net/profile/Bhupendra_Pratap_Singh/publication/282798184/figure/fig15/AS:283989639221249%401444719814399/Color-image-representation-and-RGB-matrix.png)\n\nТакож поширеними є формати **HSV** та **LAB.**","metadata":{}},{"cell_type":"markdown","source":"# Набір даних зображень\n\nНавчання моделей комп'ютерного зору вимагає великих наборів даних з міченими зображеннями. Ось кілька способів організації даних зображень:\n\n**1. Набір даних у папках зображень:**\n\n* Створіть окремі папки для кожної категорії зображень, які ви хочете, щоб ваша модель розпізнавала (наприклад, \"коти\", \"собаки\").\n* Помістіть зображення, що належать до кожної категорії, у відповідні папки.\n* Це простий та інтуїтивно зрозумілий спосіб організації невеликого набору даних.\n\n```\ndataset/\n├── cat/\n│   ├── image1.jpg\n│   ├── image2.jpg\n│   └── ...\n├── dog/\n│   ├── image3.jpg\n│   ├── image4.jpg\n│   └── ...\n└── ...  # Other classes\n```\n\n**2. Набори даних зображень з фреймом даних Pandas:**.\n\n* Використовуйте pandas, популярну бібліотеку аналізу даних у Python, для створення DataFrame.\n* Фрейм даних може мати стовпці для імен файлів зображень, міток (категорій) і потенційно додаткової інформації, як-от шляхи зображень або обмежувальні рамки (анотації для розташування об'єктів).\n* Цей підхід забезпечує більшу гнучкість у керуванні та запитах до вашого набору даних на основі міток або інших атрибутів.\n\nОсь приклад структури фрейму даних\n\n| Ім'я_зображення | Мітка |\n|---|---|\n| image1.jpg | кіт\n| image2.png | собака |\n| image3.png | машина |\n\n**3. Набори даних зображень з назвою файлу як міткою:** *Використання цього підходу передбачає, що ім'я файлу кодується у вигляді мітки.\n\n* Цей підхід передбачає, що ім'я файлу зображення кодує його мітку категорії.\n* Наприклад, зображення з назвою \"cat_001.jpg\" буде позначено як \"cat\".\n* Цей метод зручний для невеликих наборів даних, але може стати громіздким для великих наборів зі складною системою іменування.\n\n```\ndataset_folder/\n├── cat_1.jpg\n├── dog_2.png\n└── ...\n```","metadata":{}},{"cell_type":"markdown","source":"# Доповнення зображень\n\n[Візуалізація](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py)\n\nУ світі глибокого навчання, особливо в задачах комп'ютерного зору, таких як класифікація зображень і виявлення об'єктів, якість і кількість даних є вирішальними факторами, що впливають на продуктивність моделі. Однак отримання великих і різноманітних наборів даних може бути дорогим і трудомістким процесом. Саме тут на допомогу приходить доповнення зображень - потужний метод штучного розширення набору даних і підвищення надійності нейронних мереж.\n\n**Що таке доповнення зображень?**\n\nДоповнення зображень передбачає застосування серії випадкових перетворень до існуючих зображень для створення нових, дещо змінених версій. Ці зміни імітують реальні умови, з якими можуть зіткнутися зображення, наприклад, зміни в освітленні, повороті, масштабуванні та шумі. Тренуючи модель на цьому розширеному наборі даних, ви, по суті, навчаєте її ставати більш стійкою до цих змін і краще узагальнювати невидимі дані.\n\nПереваги доповнення зображень:\n\n* **Запобігає надмірному пристосуванню:** Вводячи варіації, ви не дозволяєте моделі просто запам'ятовувати навчальні дані і покращуєте її здатність вивчати основні закономірності, застосовні до нових зображень.\n* **Підвищує ефективність навчання:** Аугментація дозволяє ефективно навчати модель на меншому вихідному наборі даних, штучно розширюючи його. Це може заощадити час і ресурси, коли отримання великих наборів даних є складним завданням.\n* **Покращує стійкість моделі:** Навчання на доповнених даних піддає модель різноманітним сценаріям, роблячи її менш чутливою до помилок, спричинених незначними змінами в освітленні, перспективі або іншими факторами.\n\n**Поширені методи доповнення зображень:**.\n\nНижче наведено кілька основних методів доповнення зображень, які ви можете використовувати:\n\n* Геометричні перетворення\n    * **Поворот:** Випадковий поворот зображень на певний діапазон градусів.\n    * **Перевертання:** перевертання зображень по горизонталі або вертикалі, щоб змінити орієнтацію об'єкта.\n    * **Масштабування:** довільна зміна розміру зображень у вказаному діапазоні, щоб імітувати наближення або віддалення об'єктів.\n    * **Зсув:** обрізання зображень із різних позицій, щоб змінити розташування об'єктів у кадрі.\n    * **Зсув:** Злегка спотворює зображення по горизонталі або вертикалі для створення ефекту зсуву.\n* Перетворення колірного простору\n    * **Регулювання яскравості/контрастності:** Довільна зміна яскравості та контрастності зображень для імітації різних умов освітлення.\n    * **Колірний джиттер:** Вводить невеликі випадкові зміни в колірних каналах (відтінок, насиченість, значення), щоб імітувати природні зміни кольору.\n* Додавання шуму\n    * **Додавання випадкового гауссового шуму** до зображення для імітації шуму сенсора або дефектів зображення.\n\n**Реалізація за допомогою PyTorch:**\n\nОсь базовий приклад використання доповнення зображення за допомогою модуля `torchvision.transforms` з PyTorch:\n\n```python\nfrom torchvision import transforms\n\n# Визначаємо трансформації\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5), # Випадково перевернути по горизонталі з ймовірністю 50%\n    transforms.RandomRotation(degrees=15), # Випадковий поворот між -15 та 15 градусами\n    transforms.ToTensor(), # Перетворити зображення в тензор PyTorch\n])\n\n# Застосувати перетворення до даних зображення\naugmented_image = transform(original_image)\n```\n\n**Правильний вибір доповнень:**.\n\nЕфективність конкретних методів доповнення залежить від вашого набору даних і завдання. Ось декілька порад:\n\n* **Почніть з простого:** Почніть з базових геометричних перетворень, таких як перевертання та обертання.\n* **Для виявлення об'єктів** можуть бути корисними масштабування і зсув. Для класифікації зображень тремтіння кольору може бути менш важливим.\n* **Експериментуйте та відстежуйте:** Спробуйте різні комбінації перетворень і відстежуйте їхній вплив на ефективність навчання.\n\n**Додаткові міркування:**.\n\n* **Надмірне доповнення:** Надмірне доповнення може призвести до нереалістичних зображень і погіршити продуктивність.\n* **Баланс даних:** Переконайтеся, що ваші доповнені дані підтримують збалансоване представлення різних класів.\n* **Онлайн та офлайн доповнення:** Ви можете застосовувати доповнення онлайн під час навчання або офлайн, попередньо обробивши свій набір даних.","metadata":{}},{"cell_type":"markdown","source":"# ImageFolder\n\nКлас `ImageFolder` у PyTorch є зручним інструментом для завантаження та керування наборами зображень, які використовуються для навчання моделей глибокого навчання.\n\n**1. Налаштування ImageFolder:** * Структура набору даних\n\n* **Структура набору даних:**\n    * Організуйте ваш набір даних зображень у теки, де кожна тека представляє клас у вашій задачі класифікації. Наприклад, якщо у вас є зображення котів і собак, у вас буде дві папки з назвами \"коти\" і \"собаки\", що містять відповідні зображення.\n","metadata":{}},{"cell_type":"markdown","source":"# Імпорт бібліотек","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.589784Z","iopub.execute_input":"2024-04-10T12:54:54.590241Z","iopub.status.idle":"2024-04-10T12:54:54.594902Z","shell.execute_reply.started":"2024-04-10T12:54:54.590208Z","shell.execute_reply":"2024-04-10T12:54:54.593819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Створення ImageFolder","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/bone-break-classification-image-dataset/Bone Break Classification/Bone Break Classification'\n\n# Створіть екземпляр ImageFolder\ndataset = datasets.ImageFolder(root=data_dir)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.630058Z","iopub.execute_input":"2024-04-10T12:54:54.630329Z","iopub.status.idle":"2024-04-10T12:54:54.684173Z","shell.execute_reply.started":"2024-04-10T12:54:54.630305Z","shell.execute_reply":"2024-04-10T12:54:54.683475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Пояснення:**\n\n* Імпортуємо необхідні бібліотеки: `torch` для основних операцій PyTorch та `datasets` і `transforms` з `torchvision`.\n* `datasets.ImageFolder` використовується для створення екземпляру, що представляє набір даних.\n* Аргумент `root` вказує кореневий каталог, що містить теки ваших класів.\n\n**Розуміння атрибутів об'єкта ImageFolder:**\n\n* **`classes`:** Цей атрибут є списком, що містить назви класів в алфавітному порядку, які відповідають назвам папок у вашому наборі даних.\n* **`class_to_idx`:** Цей словник зіставляє назви класів (ключі) з відповідними цілочисельними індексами (значеннями).\n* **`imgs`:** Цей список містить кортежі, де кожен кортеж представляє зображення та відповідну мітку класу (індекс).","metadata":{}},{"cell_type":"code","source":"dataset.classes","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.685429Z","iopub.execute_input":"2024-04-10T12:54:54.685705Z","iopub.status.idle":"2024-04-10T12:54:54.691457Z","shell.execute_reply.started":"2024-04-10T12:54:54.685681Z","shell.execute_reply":"2024-04-10T12:54:54.690525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Доповнення та перетворення даних:**\n\n* Набори даних зображень часто отримують вигоду від методів доповнення даних (наприклад, випадкове обрізання, перевертання) для введення варіацій та покращення узагальнюваності моделі.\n* PyTorch пропонує модуль `transforms` для різноманітних операцій перетворення зображень.","metadata":{}},{"cell_type":"code","source":"len(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.714944Z","iopub.execute_input":"2024-04-10T12:54:54.715647Z","iopub.status.idle":"2024-04-10T12:54:54.720769Z","shell.execute_reply.started":"2024-04-10T12:54:54.715623Z","shell.execute_reply":"2024-04-10T12:54:54.71983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.759142Z","iopub.execute_input":"2024-04-10T12:54:54.759424Z","iopub.status.idle":"2024-04-10T12:54:54.770049Z","shell.execute_reply.started":"2024-04-10T12:54:54.7594Z","shell.execute_reply":"2024-04-10T12:54:54.76896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Визначити конвеєр перетворень\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)), # Зміна розміру зображення до 256x256 пікселів\n    transforms.ToTensor(), # Перетворити зображення у тензори PyTorch\n])\n\n# Створити екземпляр ImageFolder з трансформаціями\ndataset = datasets.ImageFolder(root=data_dir, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.800965Z","iopub.execute_input":"2024-04-10T12:54:54.801493Z","iopub.status.idle":"2024-04-10T12:54:54.848574Z","shell.execute_reply.started":"2024-04-10T12:54:54.801469Z","shell.execute_reply":"2024-04-10T12:54:54.847768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_tensor, label = dataset[0]\nimage_tensor.size()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.849765Z","iopub.execute_input":"2024-04-10T12:54:54.850032Z","iopub.status.idle":"2024-04-10T12:54:54.865146Z","shell.execute_reply.started":"2024-04-10T12:54:54.849992Z","shell.execute_reply":"2024-04-10T12:54:54.86427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Пояснення:**\n\n* Визначаємо послідовність трансформацій за допомогою `transforms.Compose`.\n* `transforms.Resize` змінює розмір зображення до потрібного розміру (тут 256x256).\n* `transforms.ToTensor` перетворює PIL-зображення у тензори PyTorch.\n* Оновлює екземпляр `ImageFolder` з визначеним аргументом `transform`.\n\n**Розбиття набору даних для навчання та валідації:**\n\n* Поділ вашого набору даних на навчальний та валідаційний набори має вирішальне значення для оцінювання моделі. \n* Функція `random_split` у PyTorch дозволяє випадковим чином розділити набір даних у потрібних пропорціях.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import random_split\n\ntrain_ratio = 0.8\n\n# Розділіть набір даних\ntrain_data, val_data = random_split(dataset, [train_ratio, 1-train_ratio])","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.894308Z","iopub.execute_input":"2024-04-10T12:54:54.895057Z","iopub.status.idle":"2024-04-10T12:54:54.899724Z","shell.execute_reply.started":"2024-04-10T12:54:54.895021Z","shell.execute_reply":"2024-04-10T12:54:54.898968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Зміна transformer після поділу","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((256, 256)), # Зміна розміру зображення до 256x256 пікселів\n    transforms.RandomHorizontalFlip(p=0.5), # Випадково перевернути по горизонталі з ймовірністю 50%\n    transforms.ToTensor(), # Перетворити зображення у тензори PyTorch\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((256, 256)), # Зміна розміру зображення до 256x256 пікселів\n    transforms.ToTensor(), # Перетворити зображення у тензори PyTorch\n])\n\n\nclass TransformDataset(torch.utils.data.Dataset):\n    def __init__(self, subset, transform=None):\n        self.subset = subset\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x, y = self.subset[index]\n        if self.transform:\n            x = self.transform(x)\n        return x, y\n        \n    def __len__(self):\n        return len(self.subset)\n\n    \ntrain_data = TransformDataset(train_data, transform = train_transform)\nval_data = TransformDataset(val_data, transform = test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.917298Z","iopub.execute_input":"2024-04-10T12:54:54.917547Z","iopub.status.idle":"2024-04-10T12:54:54.925395Z","shell.execute_reply.started":"2024-04-10T12:54:54.917526Z","shell.execute_reply":"2024-04-10T12:54:54.924492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data), len(val_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.949334Z","iopub.execute_input":"2024-04-10T12:54:54.949597Z","iopub.status.idle":"2024-04-10T12:54:54.95496Z","shell.execute_reply.started":"2024-04-10T12:54:54.949577Z","shell.execute_reply":"2024-04-10T12:54:54.954018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Пояснення:**\n\n* Визначаємо бажане співвідношення розбиття (`train_ratio`) для навчальних даних.\n* Функція `random_split` отримує набір даних та список довжин як аргументи. Довжини визначають кількість вибірок для кожного розбиття.\n* Створює два нових набори даних (`train_data` та `val_data`), що представляють навчальну та валідаційну вибірки відповідно.\n\n**5. Створення завантажувачів даних:**\n\n* PyTorch's `DataLoader` допомагає керувати ефективним завантаженням даних під час навчання. Він дозволяє пакетно завантажувати дані, перемішувати зразки (необов'язково) і обробляти багатопроцесорні дані (необов'язково) для пришвидшення навчання.","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n\n# Створіть завантажувачі даних\ntrain_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\nval_loader = torch.utils.data.DataLoader(val_data, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:54.994925Z","iopub.execute_input":"2024-04-10T12:54:54.995218Z","iopub.status.idle":"2024-04-10T12:54:55.000161Z","shell.execute_reply.started":"2024-04-10T12:54:54.995196Z","shell.execute_reply":"2024-04-10T12:54:54.999263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Все разом, але з уточненням","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((256, 256)), # Зміна розміру зображення до 256x256 пікселів\n    transforms.RandomHorizontalFlip(p=0.5), # Випадково перевернути по горизонталі з ймовірністю 50%\n    transforms.ToTensor(), # Перетворити зображення у тензори PyTorch\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((256, 256)), # Зміна розміру зображення до 256x256 пікселів\n    transforms.ToTensor(), # Перетворити зображення у тензори PyTorch\n])\n\n\n\n# Створити екземпляр ImageFolder з трансформаціями\ntrain_data = datasets.ImageFolder(root=data_dir, transform=train_transform,\n                                  is_valid_file=lambda path: 'Train' in path)\n\ntest_data = datasets.ImageFolder(root=data_dir, transform=test_transform,\n                                 is_valid_file=lambda path: 'Test' in path)\n\n\nbatch_size = 256\ntrain_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:55.039222Z","iopub.execute_input":"2024-04-10T12:54:55.039768Z","iopub.status.idle":"2024-04-10T12:54:55.139588Z","shell.execute_reply.started":"2024-04-10T12:54:55.039734Z","shell.execute_reply":"2024-04-10T12:54:55.138765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data), len(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:55.140953Z","iopub.execute_input":"2024-04-10T12:54:55.14125Z","iopub.status.idle":"2024-04-10T12:54:55.146722Z","shell.execute_reply.started":"2024-04-10T12:54:55.141228Z","shell.execute_reply":"2024-04-10T12:54:55.145782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Приклад створення вдасного Dataset","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\n\nclass ImageDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Custom Dataset for loading and preprocessing images.\n    \"\"\"\n    def __init__(self, root_dir, transform=None, task='Train'):\n        \"\"\"\n        Args:\n          root_dir (str): Path to the directory containing images.\n          transform (torchvision.transforms, optional): Transformations to apply to images. Defaults to None.\n        \"\"\"\n        self.root_dir = root_dir\n        self.image_paths = self.get_pathes(root_dir, task)\n\n        self.classes = os.listdir(root_dir)\n        self.class_to_idx = {label: i for i, label in enumerate(self.classes)}\n        self.transform = transform\n\n    def get_pathes(self, root, task):\n        image_paths = []\n        labels = os.listdir(root)\n\n        for label in labels:\n            images = os.listdir(os.path.join(root, label, task))    \n            image_paths.extend([os.path.join(root, label, task, image) for image in images])\n        return image_paths\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of images in the dataset.\n        \"\"\"\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Loads and preprocesses an image at a given index.\n\n        Args:\n          idx (int): Index of the image to return.\n\n        Returns:\n          tuple: A tuple containing the preprocessed image and its label (if available).\n        \"\"\"\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')  # Assuming RGB images\n        if self.transform:\n            image = self.transform(image)\n\n        # Add logic to load labels if available (modify based on your data structure)\n        label_name = os.path.normpath(image_path).split(os.path.sep)[-3]\n\n        return image, self.class_to_idx[label_name]","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:55.147927Z","iopub.execute_input":"2024-04-10T12:54:55.148254Z","iopub.status.idle":"2024-04-10T12:54:55.159558Z","shell.execute_reply.started":"2024-04-10T12:54:55.148226Z","shell.execute_reply":"2024-04-10T12:54:55.158926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = '/kaggle/input/bone-break-classification-image-dataset/Bone Break Classification/Bone Break Classification/Avulsion fracture/Train/12891_2019_2585_Fig5_HTML_png.rf.e7f1f0cb0b6723cde402057336779890.jpg'\nos.path.normpath(image_path).split(os.path.sep)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:55.176164Z","iopub.execute_input":"2024-04-10T12:54:55.176668Z","iopub.status.idle":"2024-04-10T12:54:55.182197Z","shell.execute_reply.started":"2024-04-10T12:54:55.176646Z","shell.execute_reply":"2024-04-10T12:54:55.181349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_train_data = ImageDataset(data_dir, transform=transform)\ncustom_test_data = ImageDataset(data_dir, transform=transform, task='Test')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:55.218802Z","iopub.execute_input":"2024-04-10T12:54:55.219481Z","iopub.status.idle":"2024-04-10T12:54:55.253935Z","shell.execute_reply.started":"2024-04-10T12:54:55.219457Z","shell.execute_reply":"2024-04-10T12:54:55.253278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(custom_train_data), len(custom_test_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:55.259054Z","iopub.execute_input":"2024-04-10T12:54:55.259309Z","iopub.status.idle":"2024-04-10T12:54:55.264604Z","shell.execute_reply.started":"2024-04-10T12:54:55.259289Z","shell.execute_reply":"2024-04-10T12:54:55.263719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_train_data[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:55.307832Z","iopub.execute_input":"2024-04-10T12:54:55.308456Z","iopub.status.idle":"2024-04-10T12:54:55.319887Z","shell.execute_reply.started":"2024-04-10T12:54:55.308434Z","shell.execute_reply":"2024-04-10T12:54:55.319144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Проблеми з візуалізацією\n\n* Виведення `dataset` повертає кортеж, що містить тензор зображення та відповідну йому мітку.\n* Тензор зображення має вигляд `(канали, висота, ширина)`. Цей формат поширений у фреймворках глибокого навчання, таких як PyTorch.\n* Перед побудовою зображення за допомогою `plt.imshow`, нам потрібно транспонувати канали до `(висота, ширина, канали)`, оскільки Matplotlib очікує цей формат для RGB зображень.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor i in range(3):  # Show 3 images\n    img, label = custom_train_data[i]\n\n    # Get the image data (tensor) and convert it back to a NumPy array for manipulation\n    img = img.numpy()\n\n    # Convert the color channels from (channels, height, width) to (height, width, channels) for pyplot\n    img = img.transpose((1, 2, 0))\n\n    # Get the label name from the dataset class labels\n    label_name = dataset.classes[label]\n\n    # Plot the image with a title (including label name)\n    plt.imshow(img)\n    plt.title(f\"Image: {i+1}, Label: {label_name}\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:55.356102Z","iopub.execute_input":"2024-04-10T12:54:55.356357Z","iopub.status.idle":"2024-04-10T12:54:56.271661Z","shell.execute_reply.started":"2024-04-10T12:54:55.356335Z","shell.execute_reply":"2024-04-10T12:54:56.2707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Візуалізація пакета даних","metadata":{}},{"cell_type":"code","source":"from torchvision.utils import make_grid\n\nloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=32)\n\n  \nbatch, labels = next(iter(loader))\n\ngrid = make_grid(batch).permute(1, 2, 0) # результатом є тензор\n\nplt.imshow(grid)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:56.273228Z","iopub.execute_input":"2024-04-10T12:54:56.273536Z","iopub.status.idle":"2024-04-10T12:54:57.557806Z","shell.execute_reply.started":"2024-04-10T12:54:56.27351Z","shell.execute_reply":"2024-04-10T12:54:57.556888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:57.558894Z","iopub.execute_input":"2024-04-10T12:54:57.559174Z","iopub.status.idle":"2024-04-10T12:54:57.565788Z","shell.execute_reply.started":"2024-04-10T12:54:57.559149Z","shell.execute_reply":"2024-04-10T12:54:57.564812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Згортка: розкриття локальних моделей**\n\nУявіть собі, як збільшувальне скло детектива ретельно сканує місце злочину в пошуках доказів. У комп'ютерному зорі згортка діє аналогічно. Це математична операція, яка включає фільтр (ядро), подібне до мініатюрного зображення, що ковзає по вхідному зображенню. У кожному місці, де він потрапляє, фільтр поелементно множить відповідні пікселі на зображенні та підсумовує добутки. Це генерує одне значення, що представляє реакцію фільтра на певну область зображення. Систематично переміщуючи фільтр по всьому зображенню, ми отримуємо новий двовимірний масив, який називається картою функцій.\n\n![](https://editor.analyticsvidhya.com/uploads/11178fil.gif)\n\n**Сутність згортки:**\n\n* **Feature Extraction Powerhouse:** Convolution відмінно справляється з визначенням локальних шаблонів або особливостей на зображенні. Різні фільтри, оснащені вивченими вагами, стають вправними у виявленні країв, ліній, форм та інших візуальних елементів. Ці функції стають будівельними блоками для завдань розпізнавання вищого рівня.\n* **Чемпіон ефективності параметрів:** порівняно з повністю зв’язаними шарами, де кожен нейрон одного шару з’єднується з кожним нейроном наступного, згорткові шари набагато ефективніші. Вони досягають цього шляхом розподілу вагових коефіцієнтів у фільтрі, що значно зменшує кількість параметрів для вивчення. Це не тільки покращує швидкість навчання, але й допомагає запобігти переобладнанню, поширеній проблемі нейронних мереж.","metadata":{}},{"cell_type":"markdown","source":"# **Трансформація розміру зображення**\n\nРозмір результату операції згортання залежить від кількох ключових факторів:\n\n* **Розмір фільтра має значення:** Розміри (ширина та висота) самого фільтра відіграють важливу роль у визначенні вихідного розміру. Більший фільтр захоплює ширшу область зображення, потенційно впливаючи на розмір вихідного зображення.\n* **Відступ: стратегічний контроль меж:** Відступ передбачає додавання додаткових пікселів (часто нулів) навколо зображення перед застосуванням згортки. Ця стратегічна техніка допомагає контролювати розмір вихідного матеріалу та запобігати втраті інформації на краях, яка може статися, якщо фільтр вийшов за межі зображення під час його ковзання.\n* **Кроки: кроки фільтра:** кроки визначають кількість пікселів, яку фільтр переміщує горизонтально та вертикально між обчисленнями. Крок за умовчанням дорівнює 1, тобто фільтр переміщується на один піксель за раз. Однак більші кроки можна використовувати для зменшення розміру вихідного зображення або впровадження ефекту зменшення дискретизації.\n\n**Формула вихідного розміру без заповнення:**\n\n```\nВихідна ширина = (вхідна ширина - фільтрована ширина + 1) // крок\nВихідна висота = (вхідна висота - висота фільтра + 1) // крок\n```\n\nЦя формула передбачає відсутність заповнення та враховує цілочисельний розподіл руху на крок. Наприклад, зображення розміром 32x32 з фільтром 3x3 і кроком 1 призведе до виведення карти функцій 30x30.\n\n**Доповнення: додавання стратегічного буфера**\n\nЗаповнення вирішує потенційну втрату інформації на краях зображення під час згортання. Нижче наведено найпоширеніші методи наповнення:\n\n* **Нульовий відступ:** Як випливає з назви, нульовий відступ додає нулі як додаткові межі пікселів навколо зображення. Це широко використовувана стратегія збереження просторової інформації.\n* **Reflection Padding:** цей підхід дзеркально відбиває краї зображення, щоб створити відступ. Це може бути корисно під час роботи з елементами, які можуть поширюватися на краї, такими як текстури чи візерунки.\n* **Відступ реплікації:** Тут самі краєві пікселі копіюються для створення відступу. Це може бути корисним для певних типів граничних умов.\n\n![](https://www.baeldung.com/wp-content/uploads/sites/4/2023/02/padding.png)\n\n**Доповнення та вплив на вихідний розмір:**\n\nКоли введено доповнення (позначене «p» у формулі), обчислення вихідного розміру змінюється на:\n\n```\nВихідна ширина = (вхідна ширина + 2 * p - ширина фільтра + 1) // крок\nВихідна висота = (вхідна висота + 2 * p - висота фільтра + 1) // крок\n```\n\nНаприклад, із нульовим відступом у 1 піксель (p=1) навколо зображення 28x28, фільтром 3x3 і кроком 1 розмір вихідного зображення стає 30x30. Заповнення по суті створює буферну зону навколо зображення, дозволяючи фільтру працювати без втрати інформації з оригінального зображення.\n\n![](https://saturncloud.io/images/blog/convolution-operation-on-a-mxnx3-image-matrix-with-a-3x3x3-kernel.gif)","metadata":{}},{"cell_type":"markdown","source":"## Згорткові шари: будівельні блоки для розпізнавання зображень\n\nЗгорткові шари (Conv Layers) є основними будівельними блоками згорткових нейронних мереж (CNN), типу архітектури нейронної мережі, спеціально розробленої для розпізнавання та аналізу зображень. Ось вичерпний посібник із розуміння рівня конверсії:\n\n**Концепція:**\n\n* Conv Layers застосовують фільтр (ядро) до вхідного зображення, створюючи карту функцій.\n* Фільтр ковзає по всьому зображенню (операція згортання) певними кроками, фіксуючи локальні візерунки та особливості.\n* Кілька фільтрів у межах Conv Layer можуть вивчати різні функції, як-от краї, текстури або форми об’єктів.\n* Об’єднання кількох шарів Conv із різними розмірами та кількістю фільтрів допомагає отримувати дедалі складніші функції із вхідного зображення.\n\n**Компоненти:**\n\n1. **Фільтри (ядра):** Це маленькі квадратні матриці, які вивчають ваги під час навчання. Пересуваючи фільтр по вхідних даних, Conv Layer виконує поелементне множення (скарковий добуток) між вагами фільтра та відповідними елементами у вхідному зображенні.\n2. **Ознаки:** Результатом шару згортки є тензор з ознаками. Він являє собою активацію фільтра в різних місцях вхідного зображення. Розмір тензора залежить від розміру вхідних даних, розміру фільтра, відступу та кроку.\n3. **Функція активації:** Нелінійні функції активації (наприклад, ReLU) часто застосовуються після операції згортки, щоб ввести нелінійність і покращити здатність моделі вивчати складні шаблони.\n4. **Доповнення та крок:**\n    * **Доповнення:** можна додати нулі (або інші значення) навколо меж вхідних даних, щоб контролювати розмір виведених даних. Це корисно для збереження просторової інформації під час згортання.\n    * **Крок:** Цей параметр визначає розмір кроку фільтра, коли він ковзає по входу. Крок 1 означає, що фільтр переміщує один піксель за раз, тоді як крок 2 переміщує два пікселі за раз, що призводить до меншої карти функцій.\n\n**Як працюють шари Conv:**\n\n1. **Input:** Conv Layer отримує вхідне зображення, представлене як тривимірний тензор (канали, висота, ширина) для зображень RGB.\n2. **Згортка:** кожен фільтр у шарі Conv застосовується до вхідного зображення за допомогою поелементного множення та підсумовування. Результат для кожного місця на карті об’єктів є сумою цих продуктів.\n3. **Активація:** Вихідні дані операції згортання передаються через функцію активації, наприклад ReLU, для введення нелінійності.\n4. **Кілька фільтрів:** Рівень перетворення зазвичай використовує кілька фільтрів, кожен з яких створює окрему карту функцій, що висвітлює різні аспекти вхідного зображення.\n\n**Переваги Conv Layers:**\n\n* **Локальне підключення:** Фільтри підключаються лише до невеликої області вхідних даних, зменшуючи кількість параметрів і роблячи модель ефективнішою порівняно з повністю підключеними шарами.\n* **Спільний доступ до параметрів:** той самий фільтр застосовано до всього зображення, сприяючи розподілу ваги та зменшуючи надлишок.\n* **Вилучення функцій:** Conv Layers відмінно справляються з вилученням локальних особливостей, таких як краї, текстури та форми, які є ключовими для завдань розпізнавання зображень.\n\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*u2el-HrqRPVk7x0xlvs_CA.png)","metadata":{}},{"cell_type":"markdown","source":"# **MaxPooling: захоплення найважливіших ознак**\n\nУ згорткових нейронних мережах (CNN) MaxPooling є потужною технікою для зменшення розмірності даних зі збереженням найважливіших функцій. Це досягається шляхом зменшення дискретизації вхідних карт функцій, вилучення максимального значення з попередньо визначеної прямокутної області (званої вікном об’єднання) у кожному каналі. Цей процес допомагає:\n\n- **Зменшення обчислювальних витрат**: зменшуючи кількість параметрів, MaxPooling робить вашу мережу швидшою та потребує менше пам’яті.\n- **Покращення узагальнення**: MaxPooling вводить певний ступінь інваріантності до невеликих просторових зсувів у вхідних даних, роблячи мережу менш сприйнятливою до переобладнання. Це досягається шляхом зосередження на найбільш помітних функціях, на які менш імовірно вплинуть незначні зміни позиції.\n\n**Розуміння термінології:**\n\n* **Карта вхідних функцій:** Тензор, що представляє активації зі згорткового шару, як правило, з розмірами (розмір партії, канали, висота, ширина).\n* **Вікно:** прямокутна область, яка використовується для отримання максимального значення під час MaxPooling. Зазвичай вікна мають розміри 2x2 або 3x3.\n* **Крок:** Кількість пікселів для зміщення вікна в кожному напрямку (горизонтальному та вертикальному) на вхідній карті об’єктів. Крок 1 означає, що вікно переміщується на один піксель за раз, а крок 2 означає, що воно переміщується на два пікселі, що призводить до зниження частоти дискретизації.\n* **Вихідна карта функцій:** Отриманий тензор після застосування MaxPooling зі зменшеними розмірами у висоту та ширину порівняно з вхідними (залежно від вікна та кроку).\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png)","metadata":{}},{"cell_type":"markdown","source":"# Згорткова нейромережа\n\n![](https://editor.analyticsvidhya.com/uploads/59954intro%20to%20CNN.JPG)","metadata":{}},{"cell_type":"markdown","source":"# Softmax: Перетворення сирих оцінок на ймовірності\n\nУ сфері машинного навчання, особливо в задачах багатокласової класифікації, функція softmax відіграє вирішальну роль. Вона бере вектор дійсних чисел (часто це необроблені оцінки нейронної мережі) і перетворює його у вектор ймовірностей, які в сумі дорівнюють 1. Ці ймовірності представляють ймовірність того, що вхідні дані належать до кожного можливого класу.\n\n![](https://cdn.sanity.io/images/vr8gru94/production/8e8d009b4b0b81cf63aba41be4bfd08759736d87-2048x1152.png)\n\n**Властивості виходів Softmax:**\n\n* **Значення між 0 та 1:** Кожен вихід після перетворення softmax лежить між 0 та 1, представляючи ймовірність.\n* **Найвища ймовірність вказує на передбачуваний клас:** Елемент з найвищою ймовірністю у вихідному векторі вважається найбільш ймовірним класом для вхідних даних.\n\n**Застосування Softmax:**\n\n* **Багатокласова класифікація:** Softmax є важливим у таких задачах, як розпізнавання зображень (класифікація об'єктів на зображеннях), аналіз настроїв (класифікація тексту як позитивного, негативного або нейтрального) та виявлення спаму (класифікація електронних листів як спаму або не спаму).\n* **Вихідний шар нейронних мереж:** У задачах багатокласової класифікації функція softmax зазвичай використовується як функція активації в останньому шарі нейронної мережі. Мережа виводить сирі оцінки, а softmax перетворює їх на ймовірності, які можна інтерпретувати.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass FractureClassifier(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n        self.conv5 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n        self.conv6 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n        \n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.flatten = nn.Flatten()\n        \n        self.linear1 = nn.Linear(32*6*6, 256)\n        self.linear2 = nn.Linear(256, num_classes)\n        \n\n    def forward(self, x):\n        # x - (batch, 1, 256, 256)\n        out = self.conv1(x) # (batch, 8, 254, 254)\n        out = F.relu(out)\n        \n        out = self.conv2(out) # (batch, 16, 252, 252)\n        out = F.relu(out)\n        \n        out = self.pool1(out) # (batch, 16, 126, 126)\n        \n        out = self.conv3(out) # (batch, 32, 124, 124)\n        out = F.relu(out)\n        \n        out = self.pool2(out) # (batch, 32, 62, 62)\n        \n        out = self.conv4(out) # (batch, 32, 60, 60)\n        out = F.relu(out)\n        \n        out = self.pool3(out) # (batch, 32, 30, 30)\n        \n        out = self.conv5(out) # (batch, 32, 28, 28)\n        out = F.relu(out)\n        \n        out = self.pool4(out) # (batch, 32, 14, 14)\n        \n        out = self.conv6(out) # (batch, 32, 12, 12)\n        out = F.relu(out)\n        \n        out = self.pool5(out) # (batch, 32, 6, 6)\n        \n        out = self.flatten(out) # (batch, 32*6*6)\n        \n        out = self.linear1(out)\n        out = F.relu(out)\n\n        out = self.linear2(out)\n        #out = F.softmax(out, dim=-1)\n        return out\n\n\n    def predict(self, X, device='cpu'):\n        X = torch.FloatTensor(np.array(X)).to(device)\n\n        with torch.no_grad():\n            y_pred = F.softmax(self.forward(X), dim=-1)\n\n        return y_pred.cpu().numpy()\n\n\nmodel = FractureClassifier(len(train_data.classes)).to(device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:57.568Z","iopub.execute_input":"2024-04-10T12:54:57.568338Z","iopub.status.idle":"2024-04-10T12:54:57.783781Z","shell.execute_reply.started":"2024-04-10T12:54:57.568313Z","shell.execute_reply":"2024-04-10T12:54:57.782851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Перевіряємо розмірності","metadata":{}},{"cell_type":"code","source":"!pip install -q torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:54:57.785515Z","iopub.execute_input":"2024-04-10T12:54:57.785882Z","iopub.status.idle":"2024-04-10T12:55:12.823497Z","shell.execute_reply.started":"2024-04-10T12:54:57.78585Z","shell.execute_reply":"2024-04-10T12:55:12.822394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\n\nsummary(model, input_size=(3, 256, 256))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:55:12.825063Z","iopub.execute_input":"2024-04-10T12:55:12.825389Z","iopub.status.idle":"2024-04-10T12:55:14.050024Z","shell.execute_reply.started":"2024-04-10T12:55:12.82536Z","shell.execute_reply":"2024-04-10T12:55:14.048904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Визначення функції втрат та оптимізатора\n\nloss_fn = nn.CrossEntropyLoss()\n\n# Оптимізатор (SGD) для оновлення ваг моделі\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:55:14.051492Z","iopub.execute_input":"2024-04-10T12:55:14.051763Z","iopub.status.idle":"2024-04-10T12:55:14.05754Z","shell.execute_reply.started":"2024-04-10T12:55:14.051737Z","shell.execute_reply":"2024-04-10T12:55:14.056572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Функція для тренування\nimport time\n\ndef train(model, optimizer, loss_fn, train_dl, val_dl,\n          metrics=None, metrics_name=None, epochs=20, device='cpu', task='regression'):\n    '''\n    Runs training loop for classification problems. Returns Keras-style\n    per-epoch history of loss and accuracy over training and validation data.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Neural network model\n    optimizer : torch.optim.Optimizer\n        Search space optimizer (e.g. Adam)\n    loss_fn :\n        Loss function (e.g. nn.CrossEntropyLoss())\n    train_dl :\n        Iterable dataloader for training data.\n    val_dl :\n        Iterable dataloader for validation data.\n    metrics: list\n        List of sklearn metrics functions to be calculated\n    metrics_name: list\n        List of matrics names\n    epochs : int\n        Number of epochs to run\n    device : string\n        Specifies 'cuda' or 'cpu'\n    task : string\n        type of problem. It can be regression, binary or multiclass\n\n    Returns\n    -------\n    Dictionary\n        Similar to Keras' fit(), the output dictionary contains per-epoch\n        history of training loss, training accuracy, validation loss, and\n        validation accuracy.\n    '''\n\n    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n          (type(model).__name__, type(optimizer).__name__,\n           optimizer.param_groups[0]['lr'], epochs, device))\n\n    metrics = metrics if metrics else []\n    metrics_name = metrics_name if metrics_name else [metric.__name__ for metric in metrics]\n\n    history = {} # Collects per-epoch loss and metrics like Keras' fit().\n    history['loss'] = []\n    history['val_loss'] = []\n    for name in metrics_name:\n        history[name] = []\n        history[f'val_{name}'] = []\n\n    start_time_train = time.time()\n\n    for epoch in range(epochs):\n\n        # --- TRAIN AND EVALUATE ON TRAINING SET -----------------------------\n        start_time_epoch = time.time()\n\n        model.train()\n        history_train = {name: 0 for name in ['loss']+metrics_name}\n\n        for batch in train_dl:\n            x    = batch[0].to(device)\n            y    = batch[1].to(device)\n            y_pred = model(x)\n            loss = loss_fn(y_pred, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            y_pred = y_pred.detach().cpu().numpy()\n            y = y.detach().cpu().numpy()\n\n\n            history_train['loss'] += loss.item() * x.size(0)\n            for name, func in zip(metrics_name, metrics):\n                try:\n                    history_train[name] += func(y, y_pred) * x.size(0)\n                except:\n                    if task == 'binary': y_pred_ = y_pred.round()\n                    elif task == 'multiclass': y_pred_ = y_pred.argmax(axis=-1)\n                    history_train[name] += func(y, y_pred_) * x.size(0)\n\n        for name in history_train:\n            history_train[name] /= len(train_dl.dataset)\n\n\n        # --- EVALUATE ON VALIDATION SET -------------------------------------\n        model.eval()\n        history_val = {'val_' + name: 0 for name in metrics_name+['loss']}\n\n        with torch.no_grad():\n            for batch in val_dl:\n                x    = batch[0].to(device)\n                y    = batch[1].to(device)\n                y_pred = model(x)\n                loss = loss_fn(y_pred, y)\n\n                y_pred = y_pred.cpu().numpy()\n                y = y.cpu().numpy()\n\n                history_val['val_loss'] += loss.item() * x.size(0)\n                for name, func in zip(metrics_name, metrics):\n                    try:\n                        history_val['val_'+name] += func(y, y_pred) * x.size(0)\n                    except:\n                        if task == 'binary': y_pred_ = y_pred.round()\n                        elif task == 'multiclass': y_pred_ = y_pred.argmax(axis=-1)\n\n                        history_val['val_'+name] += func(y, y_pred_) * x.size(0)\n\n        for name in history_val:\n            history_val[name] /= len(val_dl.dataset)\n\n        # PRINTING RESULTS\n\n        end_time_epoch = time.time()\n\n        for name in history_train:\n            history[name].append(history_train[name])\n            history['val_'+name].append(history_val['val_'+name])\n\n        total_time_epoch = end_time_epoch - start_time_epoch\n\n        print(f'Epoch {epoch+1:4d} {total_time_epoch:4.0f}sec', end='\\t')\n        for name in history_train:\n            print(f'{name}: {history[name][-1]:10.3g}', end='\\t')\n            print(f\"val_{name}: {history['val_'+name][-1]:10.3g}\", end='\\t')\n        print()\n\n    # END OF TRAINING LOOP\n\n    end_time_train       = time.time()\n    total_time_train     = end_time_train - start_time_train\n    print()\n    print('Time total:     %5.2f sec' % (total_time_train))\n\n    return history","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:55:14.058999Z","iopub.execute_input":"2024-04-10T12:55:14.05935Z","iopub.status.idle":"2024-04-10T12:55:14.245422Z","shell.execute_reply.started":"2024-04-10T12:55:14.05932Z","shell.execute_reply":"2024-04-10T12:55:14.244366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score\n\nhistory = train(model, optimizer, loss_fn, train_loader, test_loader,\n                epochs=20,\n                metrics=[accuracy_score],\n                device=device,\n                task='multiclass')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:55:14.246721Z","iopub.execute_input":"2024-04-10T12:55:14.247101Z","iopub.status.idle":"2024-04-10T12:58:17.189052Z","shell.execute_reply.started":"2024-04-10T12:55:14.247065Z","shell.execute_reply":"2024-04-10T12:58:17.188035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_metric(history, name):\n    plt.title(f\"Model results with {name}\")\n    plt.plot(history[name], label='train')\n    plt.plot(history['val_'+name], label='val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    plt.legend()\n\n\nplot_metric(history, 'loss')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:58:17.192664Z","iopub.execute_input":"2024-04-10T12:58:17.193162Z","iopub.status.idle":"2024-04-10T12:58:17.567724Z","shell.execute_reply.started":"2024-04-10T12:58:17.193132Z","shell.execute_reply":"2024-04-10T12:58:17.566678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(history, 'accuracy_score')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:58:17.569203Z","iopub.execute_input":"2024-04-10T12:58:17.569937Z","iopub.status.idle":"2024-04-10T12:58:17.944403Z","shell.execute_reply.started":"2024-04-10T12:58:17.569897Z","shell.execute_reply":"2024-04-10T12:58:17.943257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay\n\nmodel = model.to('cpu')  # відключаємо від gpu\n\nloader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data))\nX_test, y_test = next(iter(loader))\n\ny_pred = model.predict(X_test)\n\nConfusionMatrixDisplay.from_predictions(y_test, y_pred.argmax(-1), display_labels=test_data.classes)\nplt.xticks(rotation=90)\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:58:17.945767Z","iopub.execute_input":"2024-04-10T12:58:17.94617Z","iopub.status.idle":"2024-04-10T12:58:22.943769Z","shell.execute_reply.started":"2024-04-10T12:58:17.946134Z","shell.execute_reply":"2024-04-10T12:58:22.94282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred[0].sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:58:22.945043Z","iopub.execute_input":"2024-04-10T12:58:22.945336Z","iopub.status.idle":"2024-04-10T12:58:22.953287Z","shell.execute_reply.started":"2024-04-10T12:58:22.945311Z","shell.execute_reply":"2024-04-10T12:58:22.952374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred.argmax(-1), target_names=test_data.classes))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:58:22.954466Z","iopub.execute_input":"2024-04-10T12:58:22.954902Z","iopub.status.idle":"2024-04-10T12:58:22.984751Z","shell.execute_reply.started":"2024-04-10T12:58:22.95487Z","shell.execute_reply":"2024-04-10T12:58:22.983887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same')\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n    \n    def forward(self, x):\n        # x - (batch, in_channels, size, size)\n        out = self.conv(x) # (batch, out_channels, size, size)\n        out = self.bn(out)\n        out = F.leaky_relu(out, 0.2)\n        out = self.pool(out) # (batch, out_channels, size//2, size//2)\n        out = F.dropout(out, 0.05)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:58:22.985801Z","iopub.execute_input":"2024-04-10T12:58:22.986082Z","iopub.status.idle":"2024-04-10T12:58:22.993046Z","shell.execute_reply.started":"2024-04-10T12:58:22.986059Z","shell.execute_reply":"2024-04-10T12:58:22.992207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BetterClassifier(nn.Module):\n    def __init__(self, num_channels=16, num_classes=10):\n        super().__init__()\n        self.conv_block1 = ConvBlock(3, num_channels)\n        self.conv_block2 = ConvBlock(num_channels*1, num_channels*2)\n        self.conv_block3 = ConvBlock(num_channels*2, num_channels*4)\n        \n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv = nn.Conv2d(num_channels*4, num_channels, kernel_size=1)\n        \n        self.flatten = nn.Flatten()\n        \n        self.linear = nn.Linear(num_channels*16*16, num_classes)\n        \n\n    def forward(self, x):\n        # x - (batch, 3, 256, 256)\n        out = self.conv_block1(x)   # (batch, num_channels, 128, 128)\n        out = self.conv_block2(out) # (batch, num_channels*2, 64, 64)\n        out = self.conv_block3(out) # (batch, num_channels*4, 32, 32)\n        \n        out = self.conv(out) # (batch, num_channels, 32, 32) \n        out = self.pool(out) # (batch, num_channels, 16, 16)\n        \n        out = self.flatten(out) # (batch, num_channels*16*16)\n        \n        out = self.linear(out) # (batch, num_classes)\n        \n        return out\n\n\n    def predict(self, X, device='cpu'):\n        X = torch.FloatTensor(np.array(X)).to(device)\n\n        with torch.no_grad():\n            y_pred = F.softmax(self.forward(X), dim=-1)\n\n        return y_pred.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:59:55.464872Z","iopub.execute_input":"2024-04-10T12:59:55.465305Z","iopub.status.idle":"2024-04-10T12:59:55.475348Z","shell.execute_reply.started":"2024-04-10T12:59:55.465273Z","shell.execute_reply":"2024-04-10T12:59:55.474504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model =  BetterClassifier(num_channels=8, num_classes=len(train_data.classes)).to(device)\n\nsummary(model, input_size=(3, 256, 256))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:59:56.211832Z","iopub.execute_input":"2024-04-10T12:59:56.212532Z","iopub.status.idle":"2024-04-10T12:59:56.23226Z","shell.execute_reply.started":"2024-04-10T12:59:56.212497Z","shell.execute_reply":"2024-04-10T12:59:56.231064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\n\n# Оптимізатор (SGD) для оновлення ваг моделі\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:59:56.846319Z","iopub.execute_input":"2024-04-10T12:59:56.846821Z","iopub.status.idle":"2024-04-10T12:59:56.853536Z","shell.execute_reply.started":"2024-04-10T12:59:56.846784Z","shell.execute_reply":"2024-04-10T12:59:56.852637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = train(model, optimizer, loss_fn, train_loader, test_loader,\n                epochs=20,\n                metrics=[accuracy_score],\n                device=device,\n                task='multiclass')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:59:57.312398Z","iopub.execute_input":"2024-04-10T12:59:57.313041Z","iopub.status.idle":"2024-04-10T13:04:02.608901Z","shell.execute_reply.started":"2024-04-10T12:59:57.312994Z","shell.execute_reply":"2024-04-10T13:04:02.606912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(history, 'loss')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T13:04:02.610758Z","iopub.execute_input":"2024-04-10T13:04:02.611829Z","iopub.status.idle":"2024-04-10T13:04:02.975605Z","shell.execute_reply.started":"2024-04-10T13:04:02.611795Z","shell.execute_reply":"2024-04-10T13:04:02.974508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(history, 'accuracy_score')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T13:04:02.977162Z","iopub.execute_input":"2024-04-10T13:04:02.977573Z","iopub.status.idle":"2024-04-10T13:04:03.336162Z","shell.execute_reply.started":"2024-04-10T13:04:02.977528Z","shell.execute_reply":"2024-04-10T13:04:03.334947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to('cpu')  # відключаємо від gpu\nmodel.eval()\nloader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data))\nX_test, y_test = next(iter(loader))\n\ny_pred = model.predict(X_test)\n\nConfusionMatrixDisplay.from_predictions(y_test, y_pred.argmax(-1), display_labels=test_data.classes)\nplt.xticks(rotation=90)\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T13:04:03.338528Z","iopub.execute_input":"2024-04-10T13:04:03.338868Z","iopub.status.idle":"2024-04-10T13:04:07.03314Z","shell.execute_reply.started":"2024-04-10T13:04:03.338838Z","shell.execute_reply":"2024-04-10T13:04:07.032211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred.argmax(-1), target_names=test_data.classes))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T13:04:07.034339Z","iopub.execute_input":"2024-04-10T13:04:07.034669Z","iopub.status.idle":"2024-04-10T13:04:07.050445Z","shell.execute_reply.started":"2024-04-10T13:04:07.034639Z","shell.execute_reply":"2024-04-10T13:04:07.049485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}