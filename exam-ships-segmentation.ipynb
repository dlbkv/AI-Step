{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8290790,"sourceType":"datasetVersion","datasetId":4925026}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dlbkvv/exam-ships-segmentation?scriptVersionId=185639066\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install -q segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:19.99228Z","iopub.execute_input":"2024-05-20T11:56:19.993002Z","iopub.status.idle":"2024-05-20T11:56:32.58424Z","shell.execute_reply.started":"2024-05-20T11:56:19.992967Z","shell.execute_reply":"2024-05-20T11:56:32.582661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:32.587528Z","iopub.execute_input":"2024-05-20T11:56:32.587985Z","iopub.status.idle":"2024-05-20T11:56:44.940123Z","shell.execute_reply.started":"2024-05-20T11:56:32.58794Z","shell.execute_reply":"2024-05-20T11:56:44.938864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\nimport os\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\nfrom torchsummary import summary\nfrom segmentation_models_pytorch.losses import DiceLoss\nfrom segmentation_models_pytorch import Unet\nimport torchmetrics\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T11:56:44.94165Z","iopub.execute_input":"2024-05-20T11:56:44.941974Z","iopub.status.idle":"2024-05-20T11:56:44.949687Z","shell.execute_reply.started":"2024-05-20T11:56:44.941941Z","shell.execute_reply":"2024-05-20T11:56:44.94873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:44.951296Z","iopub.execute_input":"2024-05-20T11:56:44.951732Z","iopub.status.idle":"2024-05-20T11:56:44.961186Z","shell.execute_reply.started":"2024-05-20T11:56:44.9517Z","shell.execute_reply":"2024-05-20T11:56:44.960303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/itstep-exam2/ship_segmentations.csv')\nimg_dir = '/kaggle/input/itstep-exam2/ship_images'","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:44.964437Z","iopub.execute_input":"2024-05-20T11:56:44.964714Z","iopub.status.idle":"2024-05-20T11:56:45.515189Z","shell.execute_reply.started":"2024-05-20T11:56:44.964684Z","shell.execute_reply":"2024-05-20T11:56:45.514354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:45.516364Z","iopub.execute_input":"2024-05-20T11:56:45.516653Z","iopub.status.idle":"2024-05-20T11:56:45.527212Z","shell.execute_reply.started":"2024-05-20T11:56:45.516628Z","shell.execute_reply":"2024-05-20T11:56:45.526151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:45.528259Z","iopub.execute_input":"2024-05-20T11:56:45.528558Z","iopub.status.idle":"2024-05-20T11:56:45.824156Z","shell.execute_reply.started":"2024-05-20T11:56:45.528535Z","shell.execute_reply":"2024-05-20T11:56:45.823059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:45.825732Z","iopub.execute_input":"2024-05-20T11:56:45.826151Z","iopub.status.idle":"2024-05-20T11:56:45.875241Z","shell.execute_reply.started":"2024-05-20T11:56:45.826114Z","shell.execute_reply":"2024-05-20T11:56:45.874268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_ships_df = df.groupby('ImageId').count()\ncount_ships_df","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:45.876524Z","iopub.execute_input":"2024-05-20T11:56:45.876857Z","iopub.status.idle":"2024-05-20T11:56:46.11974Z","shell.execute_reply.started":"2024-05-20T11:56:45.876829Z","shell.execute_reply":"2024-05-20T11:56:46.11875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_img_to_ships = {}\nmax_ships_img = count_ships_df['EncodedPixels'].max()\nfor i in range(0, max_ships_img +1):\n    temp_count = count_ships_df[count_ships_df['EncodedPixels'] == i].count().iloc[0]\n    count_img_to_ships[i] = temp_count\n\ncount_img_to_ships = pd.DataFrame(list(count_img_to_ships.items()), columns=['ShipCount', 'ImageCount'])\ncount_img_to_ships","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:46.121097Z","iopub.execute_input":"2024-05-20T11:56:46.121595Z","iopub.status.idle":"2024-05-20T11:56:46.186897Z","shell.execute_reply.started":"2024-05-20T11:56:46.121558Z","shell.execute_reply":"2024-05-20T11:56:46.185929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_img_with_ships = count_img_to_ships['ImageCount'].sum() - 150_000\ntotal_img_with_ships","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:46.187992Z","iopub.execute_input":"2024-05-20T11:56:46.188248Z","iopub.status.idle":"2024-05-20T11:56:46.194315Z","shell.execute_reply.started":"2024-05-20T11:56:46.188226Z","shell.execute_reply":"2024-05-20T11:56:46.193456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.bar(count_img_to_ships['ShipCount'], count_img_to_ships['ImageCount'], edgecolor='black')\nplt.xlabel('img_count')\nplt.ylabel('ships')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:46.195618Z","iopub.execute_input":"2024-05-20T11:56:46.196203Z","iopub.status.idle":"2024-05-20T11:56:46.485638Z","shell.execute_reply.started":"2024-05-20T11:56:46.196169Z","shell.execute_reply":"2024-05-20T11:56:46.484626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.bar(count_img_to_ships['ShipCount'], count_img_to_ships['ImageCount'], edgecolor='black')\nplt.xlabel('img_count')\nplt.ylabel('ships')\nplt.ylim(0, 2000)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:46.486965Z","iopub.execute_input":"2024-05-20T11:56:46.487422Z","iopub.status.idle":"2024-05-20T11:56:46.776963Z","shell.execute_reply.started":"2024-05-20T11:56:46.487386Z","shell.execute_reply":"2024-05-20T11:56:46.776038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wout_ships = df[df['EncodedPixels'].isna()]\nwith_ships = df[df['EncodedPixels'].notna()]\n\nreduced_wout_ships = wout_ships.sample(500) \n\nbalanced_df = pd.concat([with_ships, reduced_wout_ships])\nbalanced_df","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:46.781425Z","iopub.execute_input":"2024-05-20T11:56:46.781702Z","iopub.status.idle":"2024-05-20T11:56:46.847567Z","shell.execute_reply.started":"2024-05-20T11:56:46.781679Z","shell.execute_reply":"2024-05-20T11:56:46.846629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balanced_df['EncodedPixels'] = balanced_df['EncodedPixels'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:46.848918Z","iopub.execute_input":"2024-05-20T11:56:46.849293Z","iopub.status.idle":"2024-05-20T11:56:46.857613Z","shell.execute_reply.started":"2024-05-20T11:56:46.849258Z","shell.execute_reply":"2024-05-20T11:56:46.856552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balanced_grouped = balanced_df.groupby('ImageId')['EncodedPixels'].apply(lambda x: x.tolist()).reset_index()\nbalanced_grouped","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:46.858707Z","iopub.execute_input":"2024-05-20T11:56:46.858992Z","iopub.status.idle":"2024-05-20T11:56:47.996768Z","shell.execute_reply.started":"2024-05-20T11:56:46.858969Z","shell.execute_reply":"2024-05-20T11:56:47.99572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped = balanced_grouped.groupby(balanced_grouped['EncodedPixels'].apply(lambda x: x == ['nan']))\n\nnan_group = grouped.filter(lambda x: x['EncodedPixels'].iloc[0] == ['nan'])\nnot_nan_group = grouped.filter(lambda x: x['EncodedPixels'].iloc[0] != ['nan'])\n\nreduced_not_nan_group = not_nan_group.sample(1500)\n\nrebalanced_grouped = pd.concat([reduced_not_nan_group, nan_group])\nrebalanced_grouped","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:47.998047Z","iopub.execute_input":"2024-05-20T11:56:47.998438Z","iopub.status.idle":"2024-05-20T11:56:48.045921Z","shell.execute_reply.started":"2024-05-20T11:56:47.9984Z","shell.execute_reply":"2024-05-20T11:56:48.04487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShipDataset(Dataset):\n    def __init__(self, df, image_dir, image_shape=(768,768), transform=None, preprocessing_fn=None):\n        self.df = df\n        self.image_dir = image_dir\n        self.shape = image_shape\n        self.transform = transform\n        self.preprocessing_fn = preprocessing_fn\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image_name = self.df.iloc[idx]['ImageId']\n        image_path = os.path.join(self.image_dir, image_name)\n        image = Image.open(image_path)\n        \n        rles = self.df.iloc[idx]['EncodedPixels']\n        mask = self.combine_rle_masks(rles, self.shape)\n\n        if self.transform:\n            image = self.transform(image)\n        if self.preprocessing_fn:\n            image = self.preprocessing_fn(np.array(image))\n        \n        return image, mask\n    \n\n    def rle_to_mask(self, rle, shape):\n        \"\"\"\n        converting RLE string into a mask.\n\n        Parameters:\n        rle (str): rle string (format \"start1 length1 start2 length2 ...\")\n        shape (tuple): mask shape (height, width)\n\n        Returns:\n        numpy.ndarray: mask as 2D numpy array\n        \"\"\"\n            \n        mask = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n\n        if rle == 'nan':\n            return mask.reshape(shape)\n\n        rle_nums = list(map(int, rle.split()))\n\n        starts = rle_nums[0::2]\n        lengths = rle_nums[1::2]\n\n        starts = [start -1  for start in starts]\n\n        for start, length in zip(starts, lengths):\n            mask[start:start + length] = 1\n\n        return mask.reshape(shape).T\n\n    def combine_rle_masks(self, rles, shape):\n        \"\"\"\n        Combines several RLE masks into one.\n\n        Parameters:\n        rles (list of str): list of rle strings\n        shape (tuple): mask shape (height, width)\n\n        Returns:\n        torch.Tensor: combined mask as a 2D tensor\n        \"\"\"\n        combined_mask = np.zeros(shape, dtype=np.uint8)\n\n        for rle in rles:\n            mask = self.rle_to_mask(rle, shape)\n            combined_mask = np.maximum(combined_mask, mask)\n            \n        tensor_combined_mask = torch.tensor(combined_mask, dtype=torch.uint8)\n\n        return tensor_combined_mask\n    \n   ","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.047424Z","iopub.execute_input":"2024-05-20T11:56:48.048114Z","iopub.status.idle":"2024-05-20T11:56:48.061937Z","shell.execute_reply.started":"2024-05-20T11:56:48.048077Z","shell.execute_reply":"2024-05-20T11:56:48.060913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_dataset =  ShipDataset(df=rebalanced_grouped, image_dir=img_dir)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.063183Z","iopub.execute_input":"2024-05-20T11:56:48.063568Z","iopub.status.idle":"2024-05-20T11:56:48.075673Z","shell.execute_reply.started":"2024-05-20T11:56:48.063543Z","shell.execute_reply":"2024-05-20T11:56:48.074744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_image_ids = rebalanced_grouped['ImageId'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.076808Z","iopub.execute_input":"2024-05-20T11:56:48.077096Z","iopub.status.idle":"2024-05-20T11:56:48.084957Z","shell.execute_reply.started":"2024-05-20T11:56:48.077072Z","shell.execute_reply":"2024-05-20T11:56:48.084089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ids, test_ids = train_test_split(unique_image_ids, train_size=0.8, random_state=42)\n\ntrain_df = rebalanced_grouped[rebalanced_grouped['ImageId'].isin(train_ids)]\ntest_df = rebalanced_grouped[rebalanced_grouped['ImageId'].isin(test_ids)]\n\ntrain_grouped = train_df.groupby('ImageId')['EncodedPixels'].apply(lambda x: x.tolist()).reset_index()\ntest_grouped = test_df.groupby('ImageId')['EncodedPixels'].apply(lambda x: x.tolist()).reset_index()\n\ntrain_grouped.columns = ['ImageId', 'EncodedPixels']\ntest_grouped.columns = ['ImageId', 'EncodedPixels']","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.086003Z","iopub.execute_input":"2024-05-20T11:56:48.086281Z","iopub.status.idle":"2024-05-20T11:56:48.154384Z","shell.execute_reply.started":"2024-05-20T11:56:48.086233Z","shell.execute_reply":"2024-05-20T11:56:48.153635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#избавление от ненужной размерности\ntrain_grouped['EncodedPixels'] = train_grouped['EncodedPixels'].apply(lambda x: [item for sublist in x for item in sublist])\ntest_grouped['EncodedPixels'] = test_grouped['EncodedPixels'].apply(lambda x: [item for sublist in x for item in sublist])","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.155447Z","iopub.execute_input":"2024-05-20T11:56:48.155756Z","iopub.status.idle":"2024-05-20T11:56:48.164619Z","shell.execute_reply.started":"2024-05-20T11:56:48.155729Z","shell.execute_reply":"2024-05-20T11:56:48.163599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_grouped['EncodedPixels'].iloc[6]","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.165734Z","iopub.execute_input":"2024-05-20T11:56:48.166062Z","iopub.status.idle":"2024-05-20T11:56:48.177458Z","shell.execute_reply.started":"2024-05-20T11:56:48.166028Z","shell.execute_reply":"2024-05-20T11:56:48.176478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess = get_preprocessing_fn('resnet18', pretrained='imagenet')\n\ntrain_dataset = ShipDataset(df=train_grouped, image_dir=img_dir, preprocessing_fn=preprocess)\ntest_dataset = ShipDataset(df=test_grouped, image_dir=img_dir, preprocessing_fn=preprocess)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.178757Z","iopub.execute_input":"2024-05-20T11:56:48.179114Z","iopub.status.idle":"2024-05-20T11:56:48.186474Z","shell.execute_reply.started":"2024-05-20T11:56:48.179079Z","shell.execute_reply":"2024-05-20T11:56:48.185572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    image, mask = visualize_dataset[i]\n    if isinstance(image, torch.Tensor):\n        image = image.permute(1, 2, 0).numpy()\n    if isinstance(mask, torch.Tensor):\n        mask = mask.numpy()\n\n    plt.figure(figsize=(10, 5))  \n    plt.subplot(1, 2, 1)  \n    plt.imshow(image)  \n    plt.title(\"Image\")  \n    plt.axis(\"off\")  \n    \n    plt.subplot(1, 2, 2)  \n    plt.imshow(mask, cmap='gray')  \n    plt.title(\"Mask\")  \n    plt.axis(\"off\")  \n    plt.show() \n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.187575Z","iopub.execute_input":"2024-05-20T11:56:48.188512Z","iopub.status.idle":"2024-05-20T11:56:52.385256Z","shell.execute_reply.started":"2024-05-20T11:56:48.188481Z","shell.execute_reply":"2024-05-20T11:56:52.384374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:52.386445Z","iopub.execute_input":"2024-05-20T11:56:52.386732Z","iopub.status.idle":"2024-05-20T11:56:52.391936Z","shell.execute_reply.started":"2024-05-20T11:56:52.386707Z","shell.execute_reply":"2024-05-20T11:56:52.390948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Unet(encoder_name='resnet18', \n                      encoder_depth=5, \n                      encoder_weights='imagenet', \n                      decoder_channels=(256, 128, 64, 32, 16), \n                      decoder_use_batchnorm=True,\n                      decoder_attention_type=None,\n                      in_channels=3, \n                      classes=1, \n                      activation='sigmoid')\n\nmodel = model.to(device)\n\nfor param in model.encoder.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:52.393209Z","iopub.execute_input":"2024-05-20T11:56:52.393535Z","iopub.status.idle":"2024-05-20T11:56:52.783095Z","shell.execute_reply.started":"2024-05-20T11:56:52.393511Z","shell.execute_reply":"2024-05-20T11:56:52.782348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model, input_size=(3, 768, 768))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:52.784183Z","iopub.execute_input":"2024-05-20T11:56:52.784482Z","iopub.status.idle":"2024-05-20T11:56:52.839856Z","shell.execute_reply.started":"2024-05-20T11:56:52.784457Z","shell.execute_reply":"2024-05-20T11:56:52.838982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title train function\nimport time\n\ndef train_segmentation(model, optimizer, loss_fn, train_dl, val_dl, epochs=20, device='cpu'):\n    \"\"\"\n    Trains a segmentation model using Dice Loss and Recall as the evaluation metric.\n    \n    Parameters\n    ----------\n    model : nn.Module\n        The segmentation model to train.\n    optimizer : torch.optim.Optimizer\n        The optimizer to use for training.\n    loss_fn : nn.Module\n        The loss function to use for training (DiceLoss).\n    train_dl : DataLoader\n        DataLoader for the training dataset.\n    val_dl : DataLoader\n        DataLoader for the validation dataset.\n    epochs : int, optional\n        Number of epochs to train the model. Default is 20.\n    device : str, optional\n        The device to use for training ('cpu' or 'cuda'). Default is 'cpu'.\n    \n    Returns\n    -------\n    dict\n        Dictionary containing training and validation loss and recall for each epoch.\n    \"\"\"\n    \n    recall = torchmetrics.Recall(num_classes=1, threshold=0.5, task='binary').to(device)\n\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_recall': [],\n        'val_recall': []\n    }\n    \n    start_time = time.time()\n    \n    for epoch in range(epochs):\n        epoch_start_time = time.time()\n        model.train()\n        train_loss = 0.0\n        train_recall = 0.0\n        \n        for batch in train_dl:\n            images, masks = batch\n            images, masks = images.permute(0, 3, 1, 2).to(torch.float32).to(device), masks.to(device)\n            masks = masks.unsqueeze(1)\n            optimizer.zero_grad()\n            \n            outputs = model(images)\n            loss = loss_fn(outputs, masks)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * images.size(0)\n            train_recall += recall(outputs, masks.int()).item() * images.size(0)\n        \n        train_loss /= len(train_dl.dataset)\n        train_recall /= len(train_dl.dataset)\n        \n        model.eval()\n        val_loss = 0.0\n        val_recall = 0.0\n        \n        with torch.no_grad():\n            for batch in val_dl:\n                images, masks = batch\n                images, masks = images.permute(0, 3, 1, 2).to(torch.float32).to(device), masks.to(device)\n                masks = masks.unsqueeze(1)\n                outputs = model(images)\n                loss = loss_fn(outputs, masks)\n                \n                val_loss += loss.item() * images.size(0)\n                val_recall += recall(outputs, masks.int()).item() * images.size(0)\n        \n        val_loss /= len(val_dl.dataset)\n        val_recall /= len(val_dl.dataset)\n        \n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_recall'].append(train_recall)\n        history['val_recall'].append(val_recall)\n        \n        epoch_end_time = time.time()\n        epoch_time = epoch_end_time - epoch_start_time\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Recall: {train_recall:.4f}, Val Recall: {val_recall:.4f}, Epoch Time: {epoch_time:.2f}\")\n\n    end_time = time.time()\n    total_time = end_time - start_time\n    print(f\"Training completed in {total_time:.2f} sec\")\n\n    return history","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:52.840948Z","iopub.execute_input":"2024-05-20T11:56:52.841227Z","iopub.status.idle":"2024-05-20T11:56:52.857312Z","shell.execute_reply.started":"2024-05-20T11:56:52.841202Z","shell.execute_reply":"2024-05-20T11:56:52.856368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = DiceLoss(mode='binary', from_logits=False)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:52.858484Z","iopub.execute_input":"2024-05-20T11:56:52.858766Z","iopub.status.idle":"2024-05-20T11:56:52.871442Z","shell.execute_reply.started":"2024-05-20T11:56:52.858736Z","shell.execute_reply":"2024-05-20T11:56:52.870512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = train_segmentation(model, optimizer, loss_fn, train_loader, test_loader, epochs=15, device=device)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-05-20T11:56:52.872571Z","iopub.execute_input":"2024-05-20T11:56:52.87289Z","iopub.status.idle":"2024-05-20T12:49:53.978378Z","shell.execute_reply.started":"2024-05-20T11:56:52.872859Z","shell.execute_reply":"2024-05-20T12:49:53.977351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metric(history, name):\n\n    plt.plot(history['train_'+name], label='train')\n    plt.plot(history['val_'+name], label='val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    plt.title(f'{name.capitalize()} over Epochs')\n    plt.legend()\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:49:53.979594Z","iopub.execute_input":"2024-05-20T12:49:53.979892Z","iopub.status.idle":"2024-05-20T12:49:53.985197Z","shell.execute_reply.started":"2024-05-20T12:49:53.979866Z","shell.execute_reply":"2024-05-20T12:49:53.984317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(history, 'loss')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:49:53.986197Z","iopub.execute_input":"2024-05-20T12:49:53.986472Z","iopub.status.idle":"2024-05-20T12:49:54.329632Z","shell.execute_reply.started":"2024-05-20T12:49:53.986448Z","shell.execute_reply":"2024-05-20T12:49:54.328751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(history, 'recall')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:49:54.331121Z","iopub.execute_input":"2024-05-20T12:49:54.331478Z","iopub.status.idle":"2024-05-20T12:49:54.632638Z","shell.execute_reply.started":"2024-05-20T12:49:54.33145Z","shell.execute_reply":"2024-05-20T12:49:54.631792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'ship_segmentation_model_2.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T13:10:40.545191Z","iopub.execute_input":"2024-05-20T13:10:40.546019Z","iopub.status.idle":"2024-05-20T13:10:40.696097Z","shell.execute_reply.started":"2024-05-20T13:10:40.545987Z","shell.execute_reply":"2024-05-20T13:10:40.695336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = torch.load('ship_segmentation_model_2.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T13:00:11.295174Z","iopub.execute_input":"2024-05-20T13:00:11.295951Z","iopub.status.idle":"2024-05-20T13:00:11.362471Z","shell.execute_reply.started":"2024-05-20T13:00:11.295915Z","shell.execute_reply":"2024-05-20T13:00:11.361639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_to_visualize.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-20T13:01:45.319557Z","iopub.execute_input":"2024-05-20T13:01:45.320414Z","iopub.status.idle":"2024-05-20T13:01:45.326141Z","shell.execute_reply.started":"2024-05-20T13:01:45.320381Z","shell.execute_reply":"2024-05-20T13:01:45.325202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_batch, _ = next(iter(test_loader))\n\nimages_to_visualize = images_batch[:5]\n\nloaded_model.eval()\nwith torch.no_grad():\n    outputs = loaded_model(images_to_visualize.permute(0, 3, 1, 2).to(torch.float32).to(device))\n\n\nfor image, pred in zip(images_to_visualize, outputs):\n    plt.figure(figsize=(10, 5))\n    print(image.shape)\n\n    plt.subplot(1, 2, 1)\n    plt.imshow(image.numpy())\n    plt.title('Original Image')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(pred.squeeze().cpu(), cmap='gray')\n    plt.title('Predicted Mask')\n    plt.axis('off')\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T13:10:15.837947Z","iopub.execute_input":"2024-05-20T13:10:15.838677Z","iopub.status.idle":"2024-05-20T13:10:18.464738Z","shell.execute_reply.started":"2024-05-20T13:10:15.838643Z","shell.execute_reply":"2024-05-20T13:10:18.46379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}