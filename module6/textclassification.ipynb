{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8028092,"sourceType":"datasetVersion","datasetId":4731581},{"sourceId":8214789,"sourceType":"datasetVersion","datasetId":4868922}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Механізм уваги для обробки тестів\n\n[Оригінальна стаття](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1706.03762.pdf)\n\nМеханізми уваги зробили революцію в обробці природної мови (NLP), дозволивши моделям зосередитися на найбільш релевантних частинах вхідної послідовності. У цьому посібнику розглядаються поняття уваги, багатоголової уваги та кодерів-трансформерів у PyTorch для задач NLP.\n\nРозуміння уваги:\n\nУявіть, що ви читаєте речення. Ви не приділяєте однакову увагу кожному слову, а зосереджуєтесь на найбільш важливих, щоб зрозуміти зміст. Аналогічно, увага в моделях НЛП дозволяє моделі концентруватися на певних частинах вхідної послідовності (наприклад, реченні), які є найбільш важливими для конкретного завдання, наприклад, аналізу настрою або машинного перекладу.\n\n\n<div>\n<img src=\"https://preview.redd.it/nrd3yld06rr91.png?width=761&format=png&auto=webp&s=76b11148418849a21304943898526dbdfb60052c\" width=\"500\"/>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Ключові поняття:\n\n* **Вектори запиту, ключа та значення:** Модель отримує на вхід 3 тензора: запит, ключ і значення. Вони відображають різні аспекти елементів послідовності.\n* **Ймовірність уваги:** Модель обчислює оцінку для кожної пари елементів у послідовності. Ця оцінка й відображає, наскільки важливим є один елемент (на основі його вектора ключа) для поточного елемента (на основі його вектора запиту).\n* **Зважена сума:** Використовуючи оцінки уваги, модель створює зважену суму векторів значень, ефективно фокусуючись на найбільш релевантних частинах послідовності.\n\n**Переваги уваги:** \n\n* **Довгострокові залежності:** Увага допомагає вловлювати довгострокові залежності в тексті, де віддалені слова можуть бути семантично пов'язані між собою. Це має вирішальне значення для таких завдань, як відповіді на запитання або аналіз настроїв.\n* **Розпаралелювання:** Обчислення уваги можна ефективно розпаралелити, що робить їх придатними для навчання на великих наборах даних за допомогою графічних процесорів.\n\n![](https://www.researchgate.net/publication/356271104/figure/fig5/AS:1090999354961922@1637125924695/The-general-process-of-attention-mechanism.png)","metadata":{}},{"cell_type":"markdown","source":"# Багатоголова увага\n\nСтандартний механізм уваги фокусується на одному аспекті взаємозв'язків між елементами. Багатоголова увага вирішує цю проблему, виконуючи увагу з декількох «голів», кожна з яких вивчає різні представлення взаємозв'язків. Це дозволяє моделі відображати більш глибоке розуміння вхідної послідовності.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*PiZyU-_J_nWixsTjXOUP7Q.png)","metadata":{}},{"cell_type":"markdown","source":"# Модель Трансформер\n\nТрансформаторна архітектура, популярний вибір для завдань НЛП, значною мірою покладається на механізми уваги. Кодер Transformer використовує шари багатоголової уваги з наступними мережами прямого поширення. Ці шари дозволяють моделі вивчати складні взаємозв'язки між елементами вхідної послідовності.\n\n![](https://quantdare.com/wp-content/uploads/2021/11/transformer_arch.png)","metadata":{}},{"cell_type":"markdown","source":"# Читання даних","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:32.68232Z","iopub.execute_input":"2024-04-24T13:02:32.6826Z","iopub.status.idle":"2024-04-24T13:02:36.711106Z","shell.execute_reply.started":"2024-04-24T13:02:32.682573Z","shell.execute_reply":"2024-04-24T13:02:36.7101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/bbc-full-text-document-classification/bbc_data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:36.713013Z","iopub.execute_input":"2024-04-24T13:02:36.713493Z","iopub.status.idle":"2024-04-24T13:02:37.945933Z","shell.execute_reply.started":"2024-04-24T13:02:36.713452Z","shell.execute_reply":"2024-04-24T13:02:37.944953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [Посилання на Glove](https://github.com/stanfordnlp/GloVe?tab=readme-ov-file)","metadata":{}},{"cell_type":"markdown","source":"## [Документація](https://pytorch.org/text/stable/vocab.html#vectors)","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nimport torchtext\n\n\nclass MyDataset(Dataset):\n    def __init__(self, X, y, max_len=100):\n        self.X = X\n        self.y = y\n        self.max_len = max_len\n        \n        self.label_encoder = LabelEncoder().fit(y)\n        #self.vocab = torchtext.vocab.GloVe(name='6B', dim=50)\n        self.vocab = torchtext.vocab.Vectors(\"/kaggle/input/glove6b/glove.6B.50d.txt\")\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        label = self.label_encoder.transform([self.y.iloc[idx]])\n        label = torch.tensor(label)\n        \n        text = self.X.iloc[idx]\n        tokens = text.split()\n        \n        if len(tokens) > self.max_len:\n            tokens = tokens[:self.max_len]\n        else:\n            diff = self.max_len - len(tokens)\n            \n            tokens += ['<pad>'] * diff\n        \n        X = self.vocab.get_vecs_by_tokens(tokens, lower_case_backup=True)\n        \n        return X, label[0]\n    \n    \ndataset = MyDataset(df['data'], df['labels'])","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:37.947394Z","iopub.execute_input":"2024-04-24T13:02:37.947779Z","iopub.status.idle":"2024-04-24T13:02:59.407207Z","shell.execute_reply.started":"2024-04-24T13:02:37.947744Z","shell.execute_reply":"2024-04-24T13:02:59.406331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:59.409137Z","iopub.execute_input":"2024-04-24T13:02:59.409604Z","iopub.status.idle":"2024-04-24T13:02:59.433678Z","shell.execute_reply.started":"2024-04-24T13:02:59.409579Z","shell.execute_reply":"2024-04-24T13:02:59.432811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\n\ntrain_ratio = 0.8\n\n# Розділіть набір даних\ntrain_data, test_data = random_split(dataset, [train_ratio, 1-train_ratio])","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:59.434616Z","iopub.execute_input":"2024-04-24T13:02:59.434861Z","iopub.status.idle":"2024-04-24T13:02:59.447628Z","shell.execute_reply.started":"2024-04-24T13:02:59.43484Z","shell.execute_reply":"2024-04-24T13:02:59.446929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 32\n\ntrain_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:59.448761Z","iopub.execute_input":"2024-04-24T13:02:59.449458Z","iopub.status.idle":"2024-04-24T13:02:59.46073Z","shell.execute_reply.started":"2024-04-24T13:02:59.449426Z","shell.execute_reply":"2024-04-24T13:02:59.459733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Створення моделі","metadata":{}},{"cell_type":"markdown","source":"## [Документація](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)","metadata":{}},{"cell_type":"code","source":"from torch import nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\nfrom torch import Tensor\n\n\n# Код змінено для batch_first=True\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Arguments:\n            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n        \"\"\"\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:08:57.404227Z","iopub.execute_input":"2024-04-24T13:08:57.4046Z","iopub.status.idle":"2024-04-24T13:08:57.414798Z","shell.execute_reply.started":"2024-04-24T13:08:57.404574Z","shell.execute_reply":"2024-04-24T13:08:57.413825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Документація](https://pytorch.org/docs/stable/nn.html#transformer-layers)","metadata":{}},{"cell_type":"code","source":"class TextClassifier(nn.Module):\n    def __init__(self, encoding_dim, max_len, num_classes):\n        super().__init__()\n        \n        self.pos_encoder = PositionalEncoding(d_model=encoding_dim, max_len=max_len)\n\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=encoding_dim, nhead=2, batch_first=True, dim_feedforward=64),\n            num_layers=1\n        )\n\n        self.flatten = nn.Flatten()\n\n        self.linear1 = nn.Linear(encoding_dim*max_len, num_classes)\n\n    def forward(self, x):\n        out = self.pos_encoder(x)\n        out = self.encoder(out)\n        out = self.flatten(out)\n        out = self.linear1(out)\n        return out\n\n\n    def predict(self, X, device='cpu'):\n        X = torch.FloatTensor(np.array(X)).to(device)\n\n        with torch.no_grad():\n            y_pred = F.softmax(self.forward(X), dim=-1)\n\n        return y_pred.cpu().numpy()\n\n\nmodel = TextClassifier(encoding_dim=50, max_len=100, num_classes=5).to(device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:08:59.175368Z","iopub.execute_input":"2024-04-24T13:08:59.175718Z","iopub.status.idle":"2024-04-24T13:08:59.193478Z","shell.execute_reply.started":"2024-04-24T13:08:59.175692Z","shell.execute_reply":"2024-04-24T13:08:59.192621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Кількість параметрів","metadata":{}},{"cell_type":"code","source":"params_num = 0\n\nfor param in model.parameters():\n    size = param.size()\n    \n    num = 1\n    for item in size:\n        num *= item\n    params_num += num\n\nparams_num","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:09:00.4744Z","iopub.execute_input":"2024-04-24T13:09:00.474834Z","iopub.status.idle":"2024-04-24T13:09:00.483285Z","shell.execute_reply.started":"2024-04-24T13:09:00.474801Z","shell.execute_reply":"2024-04-24T13:09:00.482111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:09:01.228698Z","iopub.execute_input":"2024-04-24T13:09:01.229577Z","iopub.status.idle":"2024-04-24T13:09:01.235643Z","shell.execute_reply.started":"2024-04-24T13:09:01.229542Z","shell.execute_reply":"2024-04-24T13:09:01.234671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef train(model, optimizer, loss_fn, train_dl, val_dl,\n          metrics=None, metrics_name=None, epochs=20, device='cpu', task='regression'):\n    '''\n    Runs training loop for classification problems. Returns Keras-style\n    per-epoch history of loss and accuracy over training and validation data.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Neural network model\n    optimizer : torch.optim.Optimizer\n        Search space optimizer (e.g. Adam)\n    loss_fn :\n        Loss function (e.g. nn.CrossEntropyLoss())\n    train_dl :\n        Iterable dataloader for training data.\n    val_dl :\n        Iterable dataloader for validation data.\n    metrics: list\n        List of sklearn metrics functions to be calculated\n    metrics_name: list\n        List of matrics names\n    epochs : int\n        Number of epochs to run\n    device : string\n        Specifies 'cuda' or 'cpu'\n    task : string\n        type of problem. It can be regression, binary or multiclass\n\n    Returns\n    -------\n    Dictionary\n        Similar to Keras' fit(), the output dictionary contains per-epoch\n        history of training loss, training accuracy, validation loss, and\n        validation accuracy.\n    '''\n\n    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n          (type(model).__name__, type(optimizer).__name__,\n           optimizer.param_groups[0]['lr'], epochs, device))\n\n    metrics = metrics if metrics else []\n    metrics_name = metrics_name if metrics_name else [metric.__name__ for metric in metrics]\n\n    history = {} # Collects per-epoch loss and metrics like Keras' fit().\n    history['loss'] = []\n    history['val_loss'] = []\n    for name in metrics_name:\n        history[name] = []\n        history[f'val_{name}'] = []\n\n    start_time_train = time.time()\n\n    for epoch in range(epochs):\n\n        # --- TRAIN AND EVALUATE ON TRAINING SET -----------------------------\n        start_time_epoch = time.time()\n\n        model.train()\n        history_train = {name: 0 for name in ['loss']+metrics_name}\n\n        for batch in train_dl:\n            x    = batch[0].to(device)\n            y    = batch[1].to(device)\n            y_pred = model(x)\n            loss = loss_fn(y_pred, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            y_pred = y_pred.detach().cpu().numpy()\n            y = y.detach().cpu().numpy()\n\n\n            history_train['loss'] += loss.item() * x.size(0)\n            for name, func in zip(metrics_name, metrics):\n              try:\n                  history_train[name] += func(y, y_pred) * x.size(0)\n              except:\n                  if task == 'binary': y_pred_ = y_pred.round()\n                  elif task == 'multiclass': y_pred_ = y_pred.argmax(axis=-1)\n                  history_train[name] += func(y, y_pred_) * x.size(0)\n\n        for name in history_train:\n            history_train[name] /= len(train_dl.dataset)\n\n\n        # --- EVALUATE ON VALIDATION SET -------------------------------------\n        model.eval()\n        history_val = {'val_' + name: 0 for name in metrics_name+['loss']}\n\n        with torch.no_grad():\n            for batch in val_dl:\n                x    = batch[0].to(device)\n                y    = batch[1].to(device)\n                y_pred = model(x)\n                loss = loss_fn(y_pred, y)\n\n                y_pred = y_pred.cpu().numpy()\n                y = y.cpu().numpy()\n\n                history_val['val_loss'] += loss.item() * x.size(0)\n                for name, func in zip(metrics_name, metrics):\n                    try:\n                        history_val['val_'+name] += func(y, y_pred) * x.size(0)\n                    except:\n                        if task == 'binary': y_pred_ = y_pred.round()\n                        elif task == 'multiclass': y_pred_ = y_pred.argmax(axis=-1)\n\n                        history_val['val_'+name] += func(y, y_pred_) * x.size(0)\n\n        for name in history_val:\n            history_val[name] /= len(val_dl.dataset)\n\n        # PRINTING RESULTS\n\n        end_time_epoch = time.time()\n\n        for name in history_train:\n            history[name].append(history_train[name])\n            history['val_'+name].append(history_val['val_'+name])\n\n        total_time_epoch = end_time_epoch - start_time_epoch\n\n        print(f'Epoch {epoch+1:4d} {total_time_epoch:4.0f}sec', end='\\t')\n        for name in history_train:\n            print(f'{name}: {history[name][-1]:10.3g}', end='\\t')\n            print(f\"val_{name}: {history['val_'+name][-1]:10.3g}\", end='\\t')\n        print()\n\n    # END OF TRAINING LOOP\n\n    end_time_train       = time.time()\n    total_time_train     = end_time_train - start_time_train\n    print()\n    print('Time total:     %5.2f sec' % (total_time_train))\n\n    return history","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:09:02.189194Z","iopub.execute_input":"2024-04-24T13:09:02.189534Z","iopub.status.idle":"2024-04-24T13:09:02.21075Z","shell.execute_reply.started":"2024-04-24T13:09:02.189507Z","shell.execute_reply":"2024-04-24T13:09:02.209766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score\n\nhistory = train(model, optimizer, loss_fn, train_loader, test_loader,\n                epochs=10,\n                metrics=[accuracy_score],\n                device=device,\n                task='multiclass')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:09:02.991971Z","iopub.execute_input":"2024-04-24T13:09:02.992289Z","iopub.status.idle":"2024-04-24T13:09:18.52472Z","shell.execute_reply.started":"2024-04-24T13:09:02.992266Z","shell.execute_reply":"2024-04-24T13:09:18.523791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Результати","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_metric(history, name):\n    plt.title(f\"Model results with {name}\")\n    plt.plot(history[name], label='train')\n    plt.plot(history['val_'+name], label='val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    plt.legend()\n\n\nplot_metric(history, 'loss')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:09:21.865866Z","iopub.execute_input":"2024-04-24T13:09:21.866632Z","iopub.status.idle":"2024-04-24T13:09:22.124778Z","shell.execute_reply.started":"2024-04-24T13:09:21.866597Z","shell.execute_reply":"2024-04-24T13:09:22.123916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(history, 'accuracy_score')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:09:23.135541Z","iopub.execute_input":"2024-04-24T13:09:23.135896Z","iopub.status.idle":"2024-04-24T13:09:23.447856Z","shell.execute_reply.started":"2024-04-24T13:09:23.135856Z","shell.execute_reply":"2024-04-24T13:09:23.447046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay\n\nmodel = model.to('cpu')  # відключаємо від gpu\n\nloader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data))\nX_test, y_test = next(iter(loader))\n\ny_pred = model.predict(X_test)\n\nConfusionMatrixDisplay.from_predictions(y_test, y_pred.argmax(-1), display_labels=dataset.label_encoder.classes_)\nplt.xticks(rotation=90)\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:25:28.718175Z","iopub.execute_input":"2024-04-24T13:25:28.719029Z","iopub.status.idle":"2024-04-24T13:25:29.440571Z","shell.execute_reply.started":"2024-04-24T13:25:28.718993Z","shell.execute_reply":"2024-04-24T13:25:29.43968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred.argmax(-1), target_names=dataset.label_encoder.classes_))","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:25:34.642469Z","iopub.execute_input":"2024-04-24T13:25:34.642803Z","iopub.status.idle":"2024-04-24T13:25:34.657372Z","shell.execute_reply.started":"2024-04-24T13:25:34.642777Z","shell.execute_reply":"2024-04-24T13:25:34.656384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}