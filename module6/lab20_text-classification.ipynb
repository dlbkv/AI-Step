{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7882142,"sourceType":"datasetVersion","datasetId":4626328},{"sourceId":8218521,"sourceType":"datasetVersion","datasetId":4871646}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-24T18:11:07.562025Z","iopub.execute_input":"2024-04-24T18:11:07.563273Z","iopub.status.idle":"2024-04-24T18:11:07.568900Z","shell.execute_reply.started":"2024-04-24T18:11:07.563224Z","shell.execute_reply":"2024-04-24T18:11:07.567914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_business = pd.read_csv(\"/kaggle/input/news-articles-classification-dataset-for-nlp-and-ml/business_data.csv\")\ndf_education = pd.read_csv(\"/kaggle/input/news-articles-classification-dataset-for-nlp-and-ml/education_data.csv\")\ndf_entertaiment = pd.read_csv(\"/kaggle/input/news-articles-classification-dataset-for-nlp-and-ml/entertainment_data.csv\")\ndf_sports = pd.read_csv(\"/kaggle/input/news-articles-classification-dataset-for-nlp-and-ml/sports_data.csv\")\ndf_technology = pd.read_csv(\"/kaggle/input/news-articles-classification-dataset-for-nlp-and-ml/technology_data.csv\")\n\n#df_business['label'] = 'business'\n#df_education['label'] = 'education'\n#df_entertaiment['label'] = 'entertainment'\n#df_sports['label'] = 'sports'\n#df_technology['label'] = 'technology'\n\ndf = pd.concat([df_business, df_education, df_entertaiment, df_sports, df_technology])\ndf['text'] = df['headlines'] + ' ' + df['description'] + ' ' + df['content']\ndf.drop(columns=['headlines', 'description', 'content', 'url'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:07.570897Z","iopub.execute_input":"2024-04-24T18:11:07.571739Z","iopub.status.idle":"2024-04-24T18:11:07.885708Z","shell.execute_reply.started":"2024-04-24T18:11:07.571704Z","shell.execute_reply":"2024-04-24T18:11:07.884645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:07.886951Z","iopub.execute_input":"2024-04-24T18:11:07.887353Z","iopub.status.idle":"2024-04-24T18:11:07.898857Z","shell.execute_reply.started":"2024-04-24T18:11:07.887320Z","shell.execute_reply":"2024-04-24T18:11:07.897777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nimport torchtext\n\n\nclass MyDataset(Dataset):\n    def __init__(self, X, y, max_len=300):\n        self.X = X\n        self.y = y\n        self.max_len = max_len\n        \n        self.label_encoder = LabelEncoder().fit(y)\n        #self.vocab = torchtext.vocab.GloVe(name='6B', dim=50)\n        self.vocab = torchtext.vocab.Vectors(\"/kaggle/input/glove6b/glove.6B.50d.txt\")\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        label = self.label_encoder.transform([self.y.iloc[idx]])\n        label = torch.tensor(label)\n        \n        text = self.X.iloc[idx]\n        tokens = text.split()\n        \n        if len(tokens) > self.max_len:\n            tokens = tokens[:self.max_len]\n        else:\n            diff = self.max_len - len(tokens)\n            \n            tokens += ['<pad>'] * diff\n        \n        X = self.vocab.get_vecs_by_tokens(tokens, lower_case_backup=True)\n        \n        return X, label[0]\n    \n    \ndataset = MyDataset(df['text'], df['category'])","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:07.901933Z","iopub.execute_input":"2024-04-24T18:11:07.902335Z","iopub.status.idle":"2024-04-24T18:11:08.574051Z","shell.execute_reply.started":"2024-04-24T18:11:07.902301Z","shell.execute_reply":"2024-04-24T18:11:08.573086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:08.575354Z","iopub.execute_input":"2024-04-24T18:11:08.576061Z","iopub.status.idle":"2024-04-24T18:11:08.586295Z","shell.execute_reply.started":"2024-04-24T18:11:08.576024Z","shell.execute_reply":"2024-04-24T18:11:08.585330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\n\ntrain_ratio = 0.8\n\n# Розділіть набір даних\ntrain_data, test_data = random_split(dataset, [train_ratio, 1-train_ratio])","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:08.587726Z","iopub.execute_input":"2024-04-24T18:11:08.588085Z","iopub.status.idle":"2024-04-24T18:11:08.594633Z","shell.execute_reply.started":"2024-04-24T18:11:08.588052Z","shell.execute_reply":"2024-04-24T18:11:08.593486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 32\n\ntrain_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:08.595978Z","iopub.execute_input":"2024-04-24T18:11:08.596826Z","iopub.status.idle":"2024-04-24T18:11:08.603241Z","shell.execute_reply.started":"2024-04-24T18:11:08.596785Z","shell.execute_reply":"2024-04-24T18:11:08.602188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\nfrom torch import Tensor\n\n\n# Код змінено для batch_first=True\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Arguments:\n            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n        \"\"\"\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:08.604505Z","iopub.execute_input":"2024-04-24T18:11:08.605140Z","iopub.status.idle":"2024-04-24T18:11:08.614772Z","shell.execute_reply.started":"2024-04-24T18:11:08.605109Z","shell.execute_reply":"2024-04-24T18:11:08.613792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextClassifier(nn.Module):\n    def __init__(self, encoding_dim, max_len, num_classes):\n        super().__init__()\n        \n        self.pos_encoder = PositionalEncoding(d_model=encoding_dim, max_len=max_len)\n\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=encoding_dim, nhead=2, batch_first=True, dim_feedforward=64),\n            num_layers=1\n        )\n\n        self.flatten = nn.Flatten()\n\n        self.linear1 = nn.Linear(encoding_dim*max_len, num_classes)\n\n    def forward(self, x):\n        out = self.pos_encoder(x)\n        out = self.encoder(out)\n        out = self.flatten(out)\n        out = self.linear1(out)\n        return out\n\n\n    def predict(self, X, device='cpu'):\n        X = torch.FloatTensor(np.array(X)).to(device)\n\n        with torch.no_grad():\n            y_pred = F.softmax(self.forward(X), dim=-1)\n\n        return y_pred.cpu().numpy()\n\n\nmodel = TextClassifier(encoding_dim=50, max_len=300, num_classes=5).to(device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:08.615939Z","iopub.execute_input":"2024-04-24T18:11:08.616225Z","iopub.status.idle":"2024-04-24T18:11:08.641576Z","shell.execute_reply.started":"2024-04-24T18:11:08.616185Z","shell.execute_reply":"2024-04-24T18:11:08.640684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_num = 0\n\nfor param in model.parameters():\n    size = param.size()\n    \n    num = 1\n    for item in size:\n        num *= item\n    params_num += num\n\nparams_num","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:08.645012Z","iopub.execute_input":"2024-04-24T18:11:08.645309Z","iopub.status.idle":"2024-04-24T18:11:08.652122Z","shell.execute_reply.started":"2024-04-24T18:11:08.645274Z","shell.execute_reply":"2024-04-24T18:11:08.651072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:08.653300Z","iopub.execute_input":"2024-04-24T18:11:08.653577Z","iopub.status.idle":"2024-04-24T18:11:08.661826Z","shell.execute_reply.started":"2024-04-24T18:11:08.653554Z","shell.execute_reply":"2024-04-24T18:11:08.660887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef train(model, optimizer, loss_fn, train_dl, val_dl,\n          metrics=None, metrics_name=None, epochs=20, device='cpu', task='regression'):\n    '''\n    Runs training loop for classification problems. Returns Keras-style\n    per-epoch history of loss and accuracy over training and validation data.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Neural network model\n    optimizer : torch.optim.Optimizer\n        Search space optimizer (e.g. Adam)\n    loss_fn :\n        Loss function (e.g. nn.CrossEntropyLoss())\n    train_dl :\n        Iterable dataloader for training data.\n    val_dl :\n        Iterable dataloader for validation data.\n    metrics: list\n        List of sklearn metrics functions to be calculated\n    metrics_name: list\n        List of matrics names\n    epochs : int\n        Number of epochs to run\n    device : string\n        Specifies 'cuda' or 'cpu'\n    task : string\n        type of problem. It can be regression, binary or multiclass\n\n    Returns\n    -------\n    Dictionary\n        Similar to Keras' fit(), the output dictionary contains per-epoch\n        history of training loss, training accuracy, validation loss, and\n        validation accuracy.\n    '''\n\n    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n          (type(model).__name__, type(optimizer).__name__,\n           optimizer.param_groups[0]['lr'], epochs, device))\n\n    metrics = metrics if metrics else []\n    metrics_name = metrics_name if metrics_name else [metric.__name__ for metric in metrics]\n\n    history = {} # Collects per-epoch loss and metrics like Keras' fit().\n    history['loss'] = []\n    history['val_loss'] = []\n    for name in metrics_name:\n        history[name] = []\n        history[f'val_{name}'] = []\n\n    start_time_train = time.time()\n\n    for epoch in range(epochs):\n\n        # --- TRAIN AND EVALUATE ON TRAINING SET -----------------------------\n        start_time_epoch = time.time()\n\n        model.train()\n        history_train = {name: 0 for name in ['loss']+metrics_name}\n\n        for batch in train_dl:\n            x    = batch[0].to(device)\n            y    = batch[1].to(device)\n            y_pred = model(x)\n            loss = loss_fn(y_pred, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            y_pred = y_pred.detach().cpu().numpy()\n            y = y.detach().cpu().numpy()\n\n\n            history_train['loss'] += loss.item() * x.size(0)\n            for name, func in zip(metrics_name, metrics):\n              try:\n                  history_train[name] += func(y, y_pred) * x.size(0)\n              except:\n                  if task == 'binary': y_pred_ = y_pred.round()\n                  elif task == 'multiclass': y_pred_ = y_pred.argmax(axis=-1)\n                  history_train[name] += func(y, y_pred_) * x.size(0)\n\n        for name in history_train:\n            history_train[name] /= len(train_dl.dataset)\n\n\n        # --- EVALUATE ON VALIDATION SET -------------------------------------\n        model.eval()\n        history_val = {'val_' + name: 0 for name in metrics_name+['loss']}\n\n        with torch.no_grad():\n            for batch in val_dl:\n                x    = batch[0].to(device)\n                y    = batch[1].to(device)\n                y_pred = model(x)\n                loss = loss_fn(y_pred, y)\n\n                y_pred = y_pred.cpu().numpy()\n                y = y.cpu().numpy()\n\n                history_val['val_loss'] += loss.item() * x.size(0)\n                for name, func in zip(metrics_name, metrics):\n                    try:\n                        history_val['val_'+name] += func(y, y_pred) * x.size(0)\n                    except:\n                        if task == 'binary': y_pred_ = y_pred.round()\n                        elif task == 'multiclass': y_pred_ = y_pred.argmax(axis=-1)\n\n                        history_val['val_'+name] += func(y, y_pred_) * x.size(0)\n\n        for name in history_val:\n            history_val[name] /= len(val_dl.dataset)\n\n        # PRINTING RESULTS\n\n        end_time_epoch = time.time()\n\n        for name in history_train:\n            history[name].append(history_train[name])\n            history['val_'+name].append(history_val['val_'+name])\n\n        total_time_epoch = end_time_epoch - start_time_epoch\n\n        print(f'Epoch {epoch+1:4d} {total_time_epoch:4.0f}sec', end='\\t')\n        for name in history_train:\n            print(f'{name}: {history[name][-1]:10.3g}', end='\\t')\n            print(f\"val_{name}: {history['val_'+name][-1]:10.3g}\", end='\\t')\n        print()\n\n    # END OF TRAINING LOOP\n\n    end_time_train       = time.time()\n    total_time_train     = end_time_train - start_time_train\n    print()\n    print('Time total:     %5.2f sec' % (total_time_train))\n\n    return history","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:08.663075Z","iopub.execute_input":"2024-04-24T18:11:08.663407Z","iopub.status.idle":"2024-04-24T18:11:08.687635Z","shell.execute_reply.started":"2024-04-24T18:11:08.663383Z","shell.execute_reply":"2024-04-24T18:11:08.686654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nhistory = train(model, optimizer, loss_fn, train_loader, test_loader,\n                epochs=20,\n                metrics=[accuracy_score],\n                device=device,\n                task='multiclass')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:11:08.689002Z","iopub.execute_input":"2024-04-24T18:11:08.689315Z","iopub.status.idle":"2024-04-24T18:17:02.776973Z","shell.execute_reply.started":"2024-04-24T18:11:08.689286Z","shell.execute_reply":"2024-04-24T18:17:02.775949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_metric(history, name):\n    plt.title(f\"Model results with {name}\")\n    plt.plot(history[name], label='train')\n    plt.plot(history['val_'+name], label='val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    plt.legend()\n\n\nplot_metric(history, 'loss')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:17:02.778544Z","iopub.execute_input":"2024-04-24T18:17:02.778952Z","iopub.status.idle":"2024-04-24T18:17:03.128370Z","shell.execute_reply.started":"2024-04-24T18:17:02.778916Z","shell.execute_reply":"2024-04-24T18:17:03.127273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(history, 'accuracy_score')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:17:03.129694Z","iopub.execute_input":"2024-04-24T18:17:03.129984Z","iopub.status.idle":"2024-04-24T18:17:03.483021Z","shell.execute_reply.started":"2024-04-24T18:17:03.129959Z","shell.execute_reply":"2024-04-24T18:17:03.482018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay\n\nmodel = model.to('cpu')  \n\nloader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data))\nX_test, y_test = next(iter(loader))\n\ny_pred = model.predict(X_test)\n\nConfusionMatrixDisplay.from_predictions(y_test, y_pred.argmax(-1), display_labels=dataset.label_encoder.classes_)\nplt.xticks(rotation=90)\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:17:03.484209Z","iopub.execute_input":"2024-04-24T18:17:03.484535Z","iopub.status.idle":"2024-04-24T18:17:09.095136Z","shell.execute_reply.started":"2024-04-24T18:17:03.484509Z","shell.execute_reply":"2024-04-24T18:17:09.094189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred.argmax(-1), target_names=dataset.label_encoder.classes_))","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:17:09.097066Z","iopub.execute_input":"2024-04-24T18:17:09.097452Z","iopub.status.idle":"2024-04-24T18:17:09.112841Z","shell.execute_reply.started":"2024-04-24T18:17:09.097418Z","shell.execute_reply":"2024-04-24T18:17:09.111718Z"},"trusted":true},"execution_count":null,"outputs":[]}]}