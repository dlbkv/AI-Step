{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlbkv/AI-Step/blob/master/module6/hw19_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Завдання 1\n",
        "Напишіть функцію, яка повертає список фраз з тексту, які відповідають певному шоблону. При необхідності можете добавити власні параметри.\n",
        "\n",
        "Протестуйте функцію на якомусь тексті."
      ],
      "metadata": {
        "id": "D56Rwjf4EXC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNjSEJ2wvlHR",
        "outputId": "519dfadc-0ed2-4298-9851-72abe240f6de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"\"\"\n",
        "Call me Ishmael. Some years ago—never mind how long precisely—having little or\n",
        "no money in my purse, and nothing particular to interest me on shore, I thought\n",
        "I would sail about a little and see the watery part of the world. It is a way I\n",
        "have of driving off the spleen and regulating the circulation. Whenever I find\n",
        "myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul;\n",
        "whenever I find myself involuntarily pausing before coffin warehouses,\n",
        "and bringing up the rear of every funeral I meet\n",
        "\"\"\"\n",
        "\n",
        "def get_phrases(text, regex, tag_name):\n",
        "  \"\"\"\n",
        "  Reurn list of phrases from text that matches regex\n",
        "\n",
        "  Params:\n",
        "    text: str - original text\n",
        "    regex: str - regular expression\n",
        "    tag_name: str - tag name that is used in nltk tree\n",
        "\n",
        "  Return:\n",
        "    phrases: list[str] - list of phrases\n",
        "  \"\"\"\n",
        "  words = word_tokenize(text)\n",
        "  words = [word for word in words if word not in string.punctuation]\n",
        "\n",
        "  tags = nltk.pos_tag(words)\n",
        "  grammar = \"%s: %s\" %(tag_name, regex)\n",
        "\n",
        "  chunk_parser = nltk.RegexpParser(grammar)\n",
        "  tree = chunk_parser.parse(tags)\n",
        "\n",
        "  sentences = []\n",
        "  for subtree in tree.subtrees(lambda t: t.label() == tag_name):\n",
        "      phrase = ''\n",
        "      for leaf in subtree.leaves():\n",
        "        phrase += f'{leaf[0]}' + ' '\n",
        "      sentences.append(phrase)\n",
        "\n",
        "  return sentences\n",
        "\n",
        "# {JJ.* NN.*} используемое регулярное выражение (на гитхабе скрывается то, что\n",
        "# заключено в символы \"больше-меньше\", потому оставлю его здесь)\n",
        "regex = '{<JJ.*><NN.*>}'\n",
        "tag_name = 'AN'\n",
        "\n",
        "sentences = get_phrases(text, regex, tag_name)\n",
        "sentences"
      ],
      "metadata": {
        "id": "G4Stff_zFQjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64b7c42-8132-4fd0-a420-c47b9a120660"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['watery part ', 'damp drizzly ']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}